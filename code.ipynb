{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8223486,"sourceType":"datasetVersion","datasetId":4875499}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mohamedarish/with-dataset-as-input?scriptVersionId=175649209\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pyspark transformers torch tqdm scikit-learn sparknlp huggingface_hub fasttext","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-04T14:59:03.301514Z","iopub.execute_input":"2024-05-04T14:59:03.301789Z","iopub.status.idle":"2024-05-04T14:59:48.630608Z","shell.execute_reply.started":"2024-05-04T14:59:03.301763Z","shell.execute_reply":"2024-05-04T14:59:48.62969Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nCollecting sparknlp\n  Downloading sparknlp-1.0.0-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.22.2)\nRequirement already satisfied: fasttext in /opt/conda/lib/python3.10/site-packages (0.9.2)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nCollecting spark-nlp (from sparknlp)\n  Downloading spark_nlp-5.3.3-py2.py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext) (2.12.0)\nRequirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from fasttext) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\nDownloading spark_nlp-5.3.3-py2.py3-none-any.whl (568 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.4/568.4 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=f8d7d71424aaba16fd539c45d07823144d520faaea3f8f4249f7c39489643548\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: spark-nlp, sparknlp, pyspark\nSuccessfully installed pyspark-3.5.1 spark-nlp-5.3.3 sparknlp-1.0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LOGISTIC REGRESSION","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nfrom huggingface_hub import hf_hub_download\nimport fasttext\nimport fasttext.util\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nlogistic_regression_model = LogisticRegression(max_iter=1000)\n\n# Step 6: Train Model in Epochs\nepochs = 1\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    logistic_regression_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, logistic_regression_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, logistic_regression_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T06:30:18.861532Z","iopub.execute_input":"2024-05-04T06:30:18.861933Z","iopub.status.idle":"2024-05-04T06:30:42.093472Z","shell.execute_reply.started":"2024-05-04T06:30:18.861903Z","shell.execute_reply":"2024-05-04T06:30:42.088598Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.6739645795565472, Train Precision: 0.6817086427052726, Train Recall: 0.6739645795565472, Train F1-score: 0.6576976785981656\nValidation Accuracy: 0.637479085331846, Validation Precision: 0.6396109859134467, Validation Recall: 0.637479085331846, Validation F1-score: 0.617633474977311\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RANDOM FOREST","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport fasttext.util\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nrandom_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Step 6: Train Model in Epochs\nepochs = 1\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    random_forest_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, random_forest_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, random_forest_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T06:30:58.140088Z","iopub.execute_input":"2024-05-04T06:30:58.141047Z","iopub.status.idle":"2024-05-04T06:32:27.113059Z","shell.execute_reply.started":"2024-05-04T06:30:58.141012Z","shell.execute_reply":"2024-05-04T06:32:27.112162Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:11<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.9995816483056756, Train Precision: 0.9995819474761473, Train Recall: 0.9995816483056756, Train F1-score: 0.999581616854672\nValidation Accuracy: 0.6296709425543782, Validation Precision: 0.6450759085951593, Validation Recall: 0.6296709425543782, Validation F1-score: 0.5991409452797823\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport fasttext\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nsvm_model = SVC(kernel='linear', random_state=42)\n\n# Step 6: Train Model in Epochs\nepochs = 1\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    svm_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, svm_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, svm_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T06:34:21.520298Z","iopub.execute_input":"2024-05-04T06:34:21.520942Z","iopub.status.idle":"2024-05-04T06:35:36.518176Z","shell.execute_reply.started":"2024-05-04T06:34:21.520908Z","shell.execute_reply":"2024-05-04T06:35:36.517263Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [01:00<?, ?it/s]\n  0%|          | 0/7171 [00:31<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.6584855668665458, Train Precision: 0.6789439172896629, Train Recall: 0.6584855668665458, Train F1-score: 0.6297087537970312\nValidation Accuracy: 0.6291132180702733, Validation Precision: 0.6349227133713751, Validation Recall: 0.6291132180702733, Validation F1-score: 0.5983744133620498\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BIDIRECTIONAL LSTM WITH ATTENTION LAYER","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Activation, Lambda, RepeatVector, Permute, Flatten\nimport tensorflow.keras.backend as K\nimport os\nimport torch\n\ndef clear_gpu_memory():\n  \"\"\"Frees memory allocated on the GPU.\"\"\"\n  if torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\"):\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n        dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\n# print(df.size)\n\ndf = df.dropna(subset=['clean_content'])\n\nprint(df.size)\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Preprocessing\nmaxlen = 10000  # Maximum sequence length\nmax_words = 200000  # Maximum number of words in vocabulary\n\n# Tokenize the text\ntokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(df['content'])\n\n# Convert text to sequences\nsequences = tokenizer.texts_to_sequences(df['content'])\n\n# Pad sequences to maxlen\nX = pad_sequences(sequences, maxlen=maxlen)\n\n# Label encoding for sentiments (assuming you have 'sentiment' column)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df['sentiment'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the Bidirectional LSTM with Attention Model\ninput_seq = Input(shape=(maxlen,))\nembedding = Embedding(max_words, 128, input_length=maxlen)(input_seq)\nlstm = Bidirectional(LSTM(64, return_sequences=True))(embedding)\n\n# Attention Mechanism\nattention = Dense(1, activation='tanh')(lstm)\nattention = Flatten()(attention)\nattention = Activation('softmax', name='attention_weights')(attention)\nattention = RepeatVector(128 * 2)(attention)\nattention = Permute([2, 1])(attention)\n\nsent_representation = Concatenate(axis=-1)([lstm, attention])\nsent_representation = Lambda(lambda x: K.sum(x, axis=1))(sent_representation)\n\noutput = Dense(1, activation='sigmoid')(sent_representation)\n\nmodel = Model(inputs=input_seq, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\nclear_gpu_memory()\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy}')\n\n# Print training and validation metrics\nprint(\"Training Loss:\", history.history['loss'])\nprint(\"Training Accuracy:\", history.history['accuracy'])\nprint(\"Validation Loss:\", history.history['val_loss'])\nprint(\"Validation Accuracy:\", history.history['val_accuracy'])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-04T06:36:20.065373Z","iopub.execute_input":"2024-05-04T06:36:20.066232Z","iopub.status.idle":"2024-05-04T06:56:53.54935Z","shell.execute_reply.started":"2024-05-04T06:36:20.066198Z","shell.execute_reply":"2024-05-04T06:56:53.548322Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"64680\n64659\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 573ms/step - accuracy: 0.4114 - loss: -25902.7051 - val_accuracy: 0.4154 - val_loss: -68951.6797\nEpoch 2/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 576ms/step - accuracy: 0.4128 - loss: -81250.4688 - val_accuracy: 0.4154 - val_loss: -120757.0312\nEpoch 3/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 578ms/step - accuracy: 0.4089 - loss: -157105.2656 - val_accuracy: 0.4154 - val_loss: -192876.8438\nEpoch 4/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 577ms/step - accuracy: 0.4001 - loss: -311555.7812 - val_accuracy: 0.4154 - val_loss: -268831.4688\nEpoch 5/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 577ms/step - accuracy: 0.4020 - loss: -418399.0312 - val_accuracy: 0.4154 - val_loss: -341662.7812\nEpoch 6/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 577ms/step - accuracy: 0.4063 - loss: -512298.0938 - val_accuracy: 0.4154 - val_loss: -398410.5938\nEpoch 7/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 577ms/step - accuracy: 0.4089 - loss: -618858.2500 - val_accuracy: 0.4154 - val_loss: -481179.6562\nEpoch 8/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 576ms/step - accuracy: 0.4114 - loss: -716554.6250 - val_accuracy: 0.4154 - val_loss: -537410.1875\nEpoch 9/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 576ms/step - accuracy: 0.4170 - loss: -798325.9375 - val_accuracy: 0.4154 - val_loss: -606501.5625\nEpoch 10/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 576ms/step - accuracy: 0.4174 - loss: -877242.3125 - val_accuracy: 0.4154 - val_loss: -654121.8125\n\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 228ms/step - accuracy: 0.4289 - loss: -728310.8125\nTest Accuracy: 0.4339826703071594\nTraining Loss: [-41889.4453125, -93974.625, -196679.625, -332648.25, -444626.625, -545343.3125, -642447.375, -742854.3125, -838400.625, -931623.75]\nTraining Accuracy: [0.4129323363304138, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636]\nValidation Loss: [-68951.6796875, -120757.03125, -192876.84375, -268831.46875, -341662.78125, -398410.59375, -481179.65625, -537410.1875, -606501.5625, -654121.8125]\nValidation Accuracy: [0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport os\nimport pandas as pd\nimport torch\n\ndef clear_gpu_memory():\n  \"\"\"Frees memory allocated on the GPU.\"\"\"\n  if torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Load Malayalam BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('l3cube-pune/malayalam-bert')\n\ndef create_malayalam_dataset(content_col, sentiment_col, max_len=128):\n    tokenizer = BertTokenizer.from_pretrained('l3cube-pune/malayalam-bert')\n\n    def tokenize_text(content, sentiment):\n        inputs = tokenizer.encode_plus(\n            content,\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            return_token_type_ids=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'text': content,\n            'input_ids': inputs['input_ids'].flatten().tolist(),\n            'attention_mask': inputs['attention_mask'].flatten().tolist(),\n            'target': int(sentiment)\n        }\n\n    return udf(tokenize_text, StructType([\n        StructField('text', StringType(), True),\n        StructField('input_ids', ArrayType(IntegerType()), True),\n        StructField('attention_mask', ArrayType(IntegerType()), True),\n        StructField('target', IntegerType(), True)\n    ]))\n\n\n# Define dataset class\nclass MalayalamDataset(Dataset):\n    def __init__(self, dataframe, max_len=128):\n        self.data = dataframe\n        self.max_len = max_len\n        self.texts = self.data.content.tolist()\n        self.targets = self.data.sentiment.tolist()\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = str(self.texts[index])\n        target = self.targets[index]\n\n        inputs = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'text': text,\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'target': torch.tensor(target, dtype=torch.long)\n        }\n\n# Load and preprocess dataset\ndef load_dataset(file_path):\n    df = pd.read_csv(file_path)\n    return df\n\n# Fine-tune BERT for sentiment analysis\ndef train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total_targets = 0\n        for batch in tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n            correct += (preds == targets.cpu().numpy()).sum()\n            total_targets += len(targets)\n\n            loss.backward()\n            optimizer.step()\n\n        avg_train_loss = total_loss / len(train_loader)\n        train_accuracy = correct / total_targets\n        print(f\"Average training loss for epoch {epoch+1}: {avg_train_loss}\")\n        print(f\"Training Accuracy for epoch {epoch+1}: {train_accuracy}\")\n\n        val_loss, val_acc, val_report = evaluate_model(model, val_loader, device)\n        print(f\"Validation loss: {val_loss}, Accuracy: {val_acc}\")\n        print(\"Validation Report:\")\n        print(val_report)\n\n        scheduler.step()\n\n# Evaluate the model\ndef evaluate_model(model, val_loader, device):\n    model.eval()\n    val_targets = []\n    val_outputs = []\n\n    total_val_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n\n            val_targets.extend(targets.cpu().numpy())\n            val_outputs.extend(preds)\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_targets, val_outputs)\n    val_report = classification_report(val_targets, val_outputs, target_names=[\"Negative\", \"Neutral\", \"Positive\"])\n    return avg_val_loss, val_accuracy, val_report\n# Main function to train and evaluate\ndef main():\n  tokenizer.save_pretrained(\"bert-tokenizer\")\n\n# Create a SparkSession\n    spark = SparkSession.builder \\\n        .appName(\"Malayalam Tweets Analysis\") \\\n        .getOrCreate()\n\n    # Read the CSV files into a Spark DataFrame\n    directory = '/kaggle/input/malayalam-tweets/'\n\n    # # List to store DataFrames from each CSV file\n    dfs = []\n\n    # # Loop through each file in the directory\n    for filename in os.listdir(directory):\n        if filename.endswith(\".csv\") and \"total\" not in filename:\n          file_path = os.path.join(directory, filename)\n          # Read the CSV file into a DataFrame\n          a_df = pd.read_csv(file_path)\n\n          if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n        dfs.append(a_df)\n\n    # # Combine all DataFrames into a single DataFrame\n    df = pd.concat(dfs, ignore_index=True)\n\n    df = df.dropna(subset=['clean_content'])\n\n    df['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n    \n    df_spark = spark.createDataFrame(df)\n\n    # Define UDF to create Malayalam Dataset\n    create_dataset_udf = create_malayalam_dataset(\"clean_content\", \"sentiment\")\n\n    # Apply UDF to create train_dataset and val_dataset\n    df_spark = df_spark.withColumn(\"malayalam_dataset\", create_dataset_udf(df_spark[\"clean_content\"], df_spark[\"sentiment\"]))\n\n    # Split dataset into train and validation\n    train_df, val_df = df_spark.randomSplit([0.9, 0.1], seed=42)\n\n    # Convert Spark DataFrame to pandas DataFrame (optional)\n    train_df_pandas = train_df.select(\"malayalam_dataset\").toPandas()\n    val_df_pandas = val_df.select(\"malayalam_dataset\").toPandas()\n\n    # Convert pandas DataFrames to datasets\n    train_dataset = [row.malayalam_dataset for _, row in train_df_pandas.iterrows()]\n    val_dataset = [row.malayalam_dataset for _, row in val_df_pandas.iterrows()]\n    \n    spark.stop()\n\n#   return\n  # # Directory containing your CSV files\n  directory = '/kaggle/input/malayalam-tweets/'\n\n  # # List to store DataFrames from each CSV file\n  dfs = []\n\n  # # Loop through each file in the directory\n  for filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n      file_path = os.path.join(directory, filename)\n      # Read the CSV file into a DataFrame\n      a_df = pd.read_csv(file_path)\n\n      if \"datetimee\" in a_df.columns:\n        # print(\"has datetimeee\")\n        a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n      # Append the DataFrame to the list\n      dfs.append(a_df)\n\n  # # Combine all DataFrames into a single DataFrame\n  df = pd.concat(dfs, ignore_index=True)\n\n  df = df.dropna(subset=['clean_content'])\n\n  df['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n  # Split dataset into train and validation\n  train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n\n  # Create datasets and dataloaders\n  train_dataset = MalayalamDataset(train_df)\n  val_dataset = MalayalamDataset(val_df)\n\n  batch_size = 16\n  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n  # Load pre-trained BERT model for sequence classification\n  model = BertForSequenceClassification.from_pretrained('l3cube-pune/malayalam-bert', num_labels=3)\n\n  # Send model to GPU, if available\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  model.to(device)\n\n  # Create optimizer\n  optimizer = AdamW(model.parameters(), lr=2e-5)\n\n  # Create scheduler\n  scheduler = ExponentialLR(optimizer, gamma=0.9)\n\n  # Train the model\n  train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=9)\n\n  clear_gpu_memory()\n\n  # Save the trained model\n  model.save_pretrained(\"malayalam_sentiment_model\")\n\nif __name__ == \"__main__\":\n  main()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T15:00:30.490788Z","iopub.execute_input":"2024-05-04T15:00:30.491292Z","iopub.status.idle":"2024-05-04T15:32:23.791269Z","shell.execute_reply.started":"2024-05-04T15:00:30.491263Z","shell.execute_reply":"2024-05-04T15:32:23.790485Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"266bee53ce4843129e7a5f5bd8f3ae8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d92b3a72227427fb84ec290d782a434"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6fecec3553c41d7ab8299ee87daaa1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e3e156ea2e4d8cb1687ae898742328"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a6bd8c923714d0c8a3078e69a2cae9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/951M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"069914876d8847b0afa403c6bed33509"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at l3cube-pune/malayalam-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/9: 100%|██████████| 520/520 [03:17<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 1: 0.8294887399444213\nTraining Accuracy for epoch 1: 0.6446529532058222\nValidation loss: 0.5921274166682671, Accuracy: 0.7608225108225108\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.62      0.81      0.70       149\n     Neutral       0.80      0.63      0.71       397\n    Positive       0.80      0.88      0.83       378\n\n    accuracy                           0.76       924\n   macro avg       0.74      0.77      0.75       924\nweighted avg       0.77      0.76      0.76       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/9: 100%|██████████| 520/520 [03:16<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 2: 0.49942260805803995\nTraining Accuracy for epoch 2: 0.8075303741128353\nValidation loss: 0.5652590314375943, Accuracy: 0.7564935064935064\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.67      0.76      0.71       149\n     Neutral       0.71      0.77      0.74       397\n    Positive       0.87      0.74      0.80       378\n\n    accuracy                           0.76       924\n   macro avg       0.75      0.76      0.75       924\nweighted avg       0.77      0.76      0.76       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/9: 100%|██████████| 520/520 [03:16<00:00,  2.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 3: 0.30031482242047786\nTraining Accuracy for epoch 3: 0.899554913990136\nValidation loss: 0.5752541442112676, Accuracy: 0.7738095238095238\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.73      0.74      0.74       149\n     Neutral       0.76      0.71      0.74       397\n    Positive       0.80      0.85      0.82       378\n\n    accuracy                           0.77       924\n   macro avg       0.76      0.77      0.77       924\nweighted avg       0.77      0.77      0.77       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/9: 100%|██████████| 520/520 [03:16<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 4: 0.16858189433383253\nTraining Accuracy for epoch 4: 0.9507999518825935\nValidation loss: 0.6793340721392426, Accuracy: 0.775974025974026\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.74      0.66      0.70       149\n     Neutral       0.73      0.79      0.76       397\n    Positive       0.84      0.81      0.82       378\n\n    accuracy                           0.78       924\n   macro avg       0.77      0.75      0.76       924\nweighted avg       0.78      0.78      0.78       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/9: 100%|██████████| 520/520 [03:16<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 5: 0.10254231956071005\nTraining Accuracy for epoch 5: 0.9748586551184891\nValidation loss: 0.808787173752127, Accuracy: 0.7640692640692641\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.82      0.56      0.67       149\n     Neutral       0.70      0.82      0.75       397\n    Positive       0.83      0.79      0.81       378\n\n    accuracy                           0.76       924\n   macro avg       0.78      0.72      0.74       924\nweighted avg       0.77      0.76      0.76       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/9: 100%|██████████| 520/520 [03:16<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 6: 0.0766541777142825\nTraining Accuracy for epoch 6: 0.9819559725730783\nValidation loss: 0.8646303641385046, Accuracy: 0.7575757575757576\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.73      0.64      0.69       149\n     Neutral       0.72      0.75      0.73       397\n    Positive       0.80      0.81      0.81       378\n\n    accuracy                           0.76       924\n   macro avg       0.75      0.73      0.74       924\nweighted avg       0.76      0.76      0.76       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/9: 100%|██████████| 520/520 [03:16<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 7: 0.05269081321938966\nTraining Accuracy for epoch 7: 0.9892938770600265\nValidation loss: 0.901808185567116, Accuracy: 0.7662337662337663\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.70      0.76      0.73       149\n     Neutral       0.73      0.75      0.74       397\n    Positive       0.84      0.78      0.81       378\n\n    accuracy                           0.77       924\n   macro avg       0.76      0.76      0.76       924\nweighted avg       0.77      0.77      0.77       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/9: 100%|██████████| 520/520 [03:16<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 8: 0.04330157337961002\nTraining Accuracy for epoch 8: 0.9904968122218213\nValidation loss: 0.9412522490682274, Accuracy: 0.7673160173160173\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.71      0.69      0.70       149\n     Neutral       0.74      0.72      0.73       397\n    Positive       0.81      0.84      0.83       378\n\n    accuracy                           0.77       924\n   macro avg       0.76      0.75      0.75       924\nweighted avg       0.77      0.77      0.77       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/9: 100%|██████████| 520/520 [03:16<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 9: 0.0351567970123142\nTraining Accuracy for epoch 9: 0.9932635630939493\nValidation loss: 0.9613229227734024, Accuracy: 0.762987012987013\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.72      0.65      0.68       149\n     Neutral       0.73      0.75      0.74       397\n    Positive       0.81      0.82      0.82       378\n\n    accuracy                           0.76       924\n   macro avg       0.75      0.74      0.75       924\nweighted avg       0.76      0.76      0.76       924\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT WITH SPARK","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import udf\nfrom pyspark.sql import SparkSession\nimport os\nimport pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport os\nimport pandas as pd\nimport torch\n\ndef clear_gpu_memory():\n  \"\"\"Frees memory allocated on the GPU.\"\"\"\n  if torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\ndef create_malayalam_dataset(content_col, sentiment_col, max_len=128):\n    tokenizer = BertTokenizer.from_pretrained('l3cube-pune/malayalam-bert')\n\n    def tokenize_text(content, sentiment):\n        inputs = tokenizer.encode_plus(\n            content,\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            return_token_type_ids=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'text': content,\n            'input_ids': inputs['input_ids'].flatten().tolist(),\n            'attention_mask': inputs['attention_mask'].flatten().tolist(),\n            'target': int(sentiment)\n        }\n\n    return udf(tokenize_text, StructType([\n        StructField('text', StringType(), True),\n        StructField('input_ids', ArrayType(IntegerType()), True),\n        StructField('attention_mask', ArrayType(IntegerType()), True),\n        StructField('target', IntegerType(), True)\n    ]))\n\n\n# Fine-tune BERT for sentiment analysis\ndef train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total_targets = 0\n        for batch in tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\"):\n\n#             input_ids = batch['input_ids'].to(device)\n            input_ids = batch[1]\n#             attention_mask = batch['attention_mask'].to(device)\n            attention_mask = batch[2]\n#             targets = batch['target'].to(device)\n            targets = batch[3]\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n            correct += (preds == targets.cpu().numpy()).sum()\n            total_targets += len(targets)\n\n            loss.backward()\n            optimizer.step()\n\n        avg_train_loss = total_loss / len(train_loader)\n        train_accuracy = correct / total_targets\n        print(f\"Average training loss for epoch {epoch+1}: {avg_train_loss}\")\n        print(f\"Training Accuracy for epoch {epoch+1}: {train_accuracy}\")\n\n        val_loss, val_acc, val_report = evaluate_model(model, val_loader, device)\n        print(f\"Validation loss: {val_loss}, Accuracy: {val_acc}\")\n        print(\"Validation Report:\")\n        print(val_report)\n\n        scheduler.step()\n\n# Evaluate the model\ndef evaluate_model(model, val_loader, device):\n    model.eval()\n    val_targets = []\n    val_outputs = []\n\n    total_val_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n#             input_ids = batch['input_ids'].to(device)\n            input_ids = batch[1]\n#             attention_mask = batch['attention_mask'].to(device)\n            attention_mask = batch[2]\n#             targets = batch['target'].to(device)\n            targets = batch[3]\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n\n            val_targets.extend(targets.cpu().numpy())\n            val_outputs.extend(preds)\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_targets, val_outputs)\n    val_report = classification_report(val_targets, val_outputs, target_names=[\"Negative\", \"Neutral\", \"Positive\"])\n    return avg_val_loss, val_accuracy, val_report\n\ndef main():\n    # Create a SparkSession\n    spark = SparkSession.builder \\\n        .appName(\"Malayalam Tweets Analysis\") \\\n        .getOrCreate()\n\n    # Read the CSV files into a Spark DataFrame\n    directory = '/kaggle/input/malayalam-tweets/'\n\n    # # List to store DataFrames from each CSV file\n    dfs = []\n\n    # # Loop through each file in the directory\n    for filename in os.listdir(directory):\n        if filename.endswith(\".csv\") and \"total\" not in filename:\n          file_path = os.path.join(directory, filename)\n          # Read the CSV file into a DataFrame\n          a_df = pd.read_csv(file_path)\n\n          if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n        dfs.append(a_df)\n\n    # # Combine all DataFrames into a single DataFrame\n    df = pd.concat(dfs, ignore_index=True)\n\n    df = df.dropna(subset=['clean_content'])\n\n    df['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n    \n    df_spark = spark.createDataFrame(df)\n\n    # Define UDF to create Malayalam Dataset\n    create_dataset_udf = create_malayalam_dataset(\"clean_content\", \"sentiment\")\n\n    # Apply UDF to create train_dataset and val_dataset\n    df_spark = df_spark.withColumn(\"malayalam_dataset\", create_dataset_udf(df_spark[\"clean_content\"], df_spark[\"sentiment\"]))\n\n    # Split dataset into train and validation\n    train_df, val_df = df_spark.randomSplit([0.9, 0.1], seed=42)\n\n    # Convert Spark DataFrame to pandas DataFrame (optional)\n    train_df_pandas = train_df.select(\"malayalam_dataset\").toPandas()\n    val_df_pandas = val_df.select(\"malayalam_dataset\").toPandas()\n\n    # Convert pandas DataFrames to datasets\n    train_dataset = [row.malayalam_dataset for _, row in train_df_pandas.iterrows()]\n    val_dataset = [row.malayalam_dataset for _, row in val_df_pandas.iterrows()]\n    # Other steps (not included in this snippet)\n    # Initialize model, optimizer, scheduler\n    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    # val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    # train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=10)\n    \n    print(\"Split done\")\n    \n    batch_size = 16\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    print(\"Loaders loaded\")\n\n    # Load pre-trained BERT model for sequence classification\n    model = BertForSequenceClassification.from_pretrained('l3cube-pune/malayalam-bert', num_labels=3)\n    \n    print(\"Model loaded\")\n\n    # Send model to GPU, if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    print(\"GPU done\")\n\n    # Create optimizer\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    \n    print(\"Optimizer loaded\")\n\n    # Create scheduler\n    scheduler = ExponentialLR(optimizer, gamma=0.9)\n    \n    print(\"Scheduler loaded\")\n\n    # Train the model\n    train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=9)\n    \n    print(\"Model trained\")\n\n    clear_gpu_memory()\n    \n    print(\"gpu memory cleared\")\n    \nif __name__ == \"__main__\":\n  main()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-04T15:40:38.393941Z","iopub.execute_input":"2024-05-04T15:40:38.394294Z","iopub.status.idle":"2024-05-04T15:40:55.336341Z","shell.execute_reply.started":"2024-05-04T15:40:38.394268Z","shell.execute_reply":"2024-05-04T15:40:55.335013Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"24/05/04 15:40:40 WARN TaskSetManager: Stage 10 contains a task of very large size (1951 KiB). The maximum recommended task size is 1000 KiB.\n24/05/04 15:40:46 WARN TaskSetManager: Stage 11 contains a task of very large size (1951 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Split done\nLoaders loaded\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at l3cube-pune/malayalam-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Model loaded\nGPU done\nOptimizer loaded\nScheduler loaded\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/9:   0%|          | 0/524 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[('ആർക്ക് സാക്ഷരത  ഇസ്ലാമിക തീവ്രവാദികളുടെ ആസനം  താങ്ങുന്ന ഇടത് വലത് രാഷ്രീയ നപുസ്ഥകങ്ങൾക്കോ  തീവ്രവാദിയെ പിടിക്കാൻ ഡൽഹിയിൽ  നിന്നും  പ്രതേക വ്യാമസേനാ വിമാനം ഇറക്കിയ സംസ്ഥാന മാണ്', 'നമുക്ക് ഇഷ്ടപെട്ട ആളെ കെട്ടിയില്ലെങ്കിൽ കെട്ടുന്ന പെണ്ണിനെ കഷ്ട പെട്ടു ഇഷ്ടപെടേണ്ടിവരും by മൂവി', '2019 ല്\\u200d വീണ്ടും അധികാരത്തിലേറിയ ശേഷമാണ് മോദി സര്\\u200dക്കാര്\\u200d പരിസ്ഥിതി ആഘാത വിലയിരുത്തലില്\\u200d ഭേദഗതിക്കായി കരടു തയ്യാറാക്കിയത്.യുപിഎ സര്\\u200dക്കാരിന്റെ കാലത്ത് അന്ന് പരിസ്ഥിതി മന്ത്രിയായിരുന്ന ജയറാം രമേശ് തയ്യാറാക്കിയിരുന്ന പരിസ്ഥിതി കരടു ഭേദഗതികള്\\u200d പരിസ്ഥിതി മൗലികവാദത്തില്\\u200d', 'സേവാഭാരതിയുടെ ഒരാഴ്ച്ചത്തെ കഠിനാദ്ധ്വാനം നാടിന് സമര്\\u200dപ്പിച്ചു; കോമളം കടവിലെ പാലം ഇരുകരതൊട്ടു; അഭിനന്ദന പ്രവാഹവുമായി നാട്ടുകാര്\\u200d', 'ഇത് പേടി അല്ല... ജാഗ്രത മാത്രം...   ബാലൻ സാറിന് അങ്ങനെ ഒക്കെ പറയാം...  ബാലൻ സാറിന് വേർഡിനെ പറ്റി, എക്സലിനെ പറ്റി, എക്സലോജിക്കിനെ പറ്റി എന്ത് അറിയാം..  പിടിക്കപ്പെട്ടാൽ പെടുന്നതും വീണ് പോകുന്നതും വീണ അല്ല്യോ ബാലൻ സാറേ...  ഒക്കെ പെൻഷൻ കാശ് ആണ് - ആകെ ഉള്ള ഒരു സമാധാനം', 'ഒരു ചിൽഡ് ബിയറടിക്കാൻ ശക്തമായ ത്വര ഉണ്ടായിട്ടും ക്രമസമാധാനം തകരാതിരിക്കാൻ വേണ്ടി ഒരു ഗ്ലാസ് ചൂട്വെള്ളമടിച് സമാധാനം അഭിനയിചിങ്ങനിരിക്കുവാണ്..', 'അസാമാന്യ നേതൃശേഷിയുള്ള വ്യക്തിത്വം ഉത്തമ മാതൃക  -മുഖ്യമന്ത്രി പിണറായി വിജയൻ', 'ഇത് പിന്നെ അഴിമതി അല്ലാത്തത് കൊണ്ട് സമാധാനം ഉണ്ട്', 'തന്\\u200dറെ \\xa0മേൽ ഒരു ഉന്നത അസ്തിത്വം ഉണ്ടെന്നും അവൻ തനിക്കുവേണ്ടി മാത്രമല്ല സമാധാനം ആഗ്രഹിക്കുന്നതെന്നും പ്രസ്തുത മുഴുവൻ ലോകത്തിനു വേണ്ടിയും ആണ് ആഗ്രഹിക്കുന്നത് എന്നും മനസ്സിലാക്കുമ്പോഴാണ് സമാധാനം സ്ഥാപിക്കാൻ കഴിയുക.', 'കിറ്റ് വാങ്ങി വോട്ട് കുത്തിയ കേരള വോട്ടർമാർക്ക് അഭിമാന നിമിഷം. നമ്മുടെ പിണറായിയും ഉണ്ട് കൂട്ടത്തിൽ', 'കാപ്പിയില്ലാത്ത ഒരു ദിനം ലോകത്തിലെ ഭൂരിഭാഗം ആളുകള്\\u200dക്കും ചിന്തിക്കുവാന്\\u200d കഴിയില്ല. ഉന്മേഷം പകരുന്ന ഒരു പാനീയം എന്നതിലുപരിയായി കാപ്പി കുടിക്കുന്നത് ജീവിചര്യയുടെ ഒരു ഭാഗം തന്നെയാണ്', 'മനസ്സു കൊണ്ട്\\u200c പറയാനാകും,  \"Technically most perfect film ever made in this country\"  - Priyadarshan  അത് അടിവര ഇടുന്ന അതിഗംഭീര shots', 'Kabir is God  കലിയുഗത്തിന്റെ ആദ്യഘട്ടത്തിന്റെ അവസാന ഭാഗത്തിൽ  ഞാൻ മഘറിൽ ഒരു ലീല ചെയ്യുമെന്ന് ദൈവം കബീർ ജി പറഞ്ഞിട്ടുണ്ട്. ഞാൻ ബ്രാഹ്മണരുമായി ആത്മീയ വിജ്ഞാനം ചർച്ച ചെയ്യും.', 'തിരഞ്ഞെടുപ്പ് പ്രക്രിയയിലെ പങ്കാളിത്തം വർദ്ധിപ്പിക്കണമെന്ന് രാജ്യത്തെ യുവജനങ്ങളോട് ആഹ്വാനം ചെയ്ത്   പ്രധാനമന്ത്രി നരേന്ദ്രമോദി.ഭാരതത്തിലെ സ്ത്രീ ശക്തിയെ അഭിനന്ദിച്ച് മന്\\u200dകി ബാത്തില്\\u200d പ്രധാനമന്ത്രി.', 'പ്രവാചകാനുരാഗത്തില്\\u200d ലയിച്ച് ഇശ്ഖേ മദീന സ്നേഹ സംഗമം സമാപിച്ചു', 'തന്റെ ഗൂഢലക്ഷ്യങ്ങൾ നടപ്പിലാക്കാൻ അനഘ. മാംഗല്യ മറക്കാതെ കാണുക, Zee5 പ്രേക്ഷകരുടെ ഇഷ്ട പരമ്പര.'), [tensor([104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104,\n        104, 104]), tensor([  5304,  30660,   2666,   1023,   1831,   1201,    977,   1831,   2496,\n          7081, 111085,  81786,  54470,  36946,  57399,   2496]), tensor([  1762,    979,  29678,   6421, 129233, 108502,  54202,  17727,  34941,\n          4147, 103934,   1703,   1121,  47747, 193894,    992]), tensor([ 76473,  66013,   6969,  32227,   7062,   1782,  61402,  79039,   1201,\n         40746,   1201,  29822,   4522,   2425, 148625,   8932]), tensor([ 27921,  64870,  89765,  30709,  41300,   3439,   2598, 148730,  35318,\n         65820,  26192,   4823,  41049, 113243,  38104, 184554]), tensor([135186, 174568,   8632,  54677,    121,   3482, 143382,   1941, 104887,\n        124844,  13710, 184742,  28844, 105413, 138905,  85911]), tensor([ 33416, 197006,  39587,  98302,    121, 112282,  84503,   4823, 180722,\n          2598,  48020,  72603,   2703, 143646,   1017,  38964]), tensor([   978,   7149,  25165,    987,    121, 165375,  56428,  74953, 171543,\n          3216, 120441, 103917,  44090,  26165,  92722,   2945]), tensor([ 72247,  52577,  33728,  38104,    997,  17635,   6454,   5289,   1611,\n          1020,   3652,    119,   3766,  98121,    979, 184613]), tensor([ 79094,   2402,  61932,  65267, 175310,  50544, 135395,    105,  30367,\n         30267,  73721,    109, 114023,  19388,  40188,    977]), tensor([ 70556,  75691,   9511,  65049,   5616,  73820, 115160,      0,  84096,\n         46881,  38258,  25172,   3018, 108841,  46224,   4443]), tensor([102329,  52640,  54031,   3365,    121,  35940,  57670,      0,  47286,\n         62908,  41211,   1691,  16017,  53009,   6421,  62503]), tensor([148384, 111724,  24430,    990,    121,  30999,    120,      0,  15083,\n           977,  79633,   1936, 150728,  68200, 109897,    121]), tensor([141262,    990,    978, 163441,    121,  49802,  23334,      0,  74953,\n        116547,  66568,  12651,  28991,  10295,   3095,  21527]), tensor([158470,  66013,  62503,  37458,  60423,  84751,  55164,      0, 184558,\n         71120,    121,   1827,  10568,  25386,  77740, 112114]), tensor([  2598,  21677,  84344,  58536, 152228,   8318,  52827,      0,  41886,\n        109196,    981,   7156,   1013,  73489,  19362, 124289]), tensor([  1008,   2703,  22349, 164336,  17532,  38643,    105,      0,  36115,\n           121, 133965,   1872,  62503,   9317, 170661,   1013]), tensor([ 70721,    979,  15494,  34858,   9933, 152637,      0,      0,  11309,\n         16815,   6421,   1112,  14424,  19822,   1023, 123539]), tensor([112958,  66013,  86067,  53537,  38369,   1004,      0,      0, 196963,\n         55164,  45720,   1222,   1201,    121,  38643,  46157]), tensor([ 21695,  20132,  45158,  63763,  32039,  32998,      0,      0,  89829,\n          1468, 162072,   2398,  86204,  56154,  53856,   6051]), tensor([27931,  4874, 15038, 69177,   121, 84344,     0,     0,  1796,  5289,\n         1201,   109, 38763, 20110,   105,   119]), tensor([135186, 103588,  76348,  18403,    121,  97863,      0,      0, 184558,\n         60713,  75421,    120, 173660,  24195,      0,  88252]), tensor([ 63772,  27616,  14238,    134,    121,   8887,      0,      0,   1941,\n           105, 145183,  92284,  25735,   2418,      0,   2662]), tensor([156135,  24086,  94598,  42728,  60423,   6024,      0,      0,   3463,\n             0,   7252,  64519, 183326,    977,      0,  57399]), tensor([103049,   1156,  10705, 181869, 152228,   1201,      0,      0,  81786,\n             0,   1308,   3494,   5379, 116547,      0, 187505]), tensor([  2038,  73789, 129489,   1611,  17532,  83982,      0,      0, 182958,\n             0,   7552,  14494, 105308, 116580,      0,  46174]), tensor([ 57399,    105,    121,    990,  47625,  74594,      0,      0,  46233,\n             0, 163963,  73820,    121,  16169,      0,    979]), tensor([ 63026,      0,   5268,   8828,  76659, 141087,      0,      0,   5053,\n             0,   4155,    979,  10568, 169240,      0,  66013]), tensor([  3933,      0,  15716,  39763,  36554,   5833,      0,      0, 105223,\n             0, 111085,  20710,  87926,  12846,      0,  39706]), tensor([ 92667,      0,  36134,  32995,    119,   7062,      0,      0,  74953,\n             0,  64150,  88858,  60194, 125096,      0,    121]), tensor([ 24791,      0,   9511,  43060,  41126,  64896,      0,      0,  94564,\n             0, 102581,  21038,  51275,   8477,      0,    105]), tensor([  5833,      0, 111554,  32998,   8318,  74953,      0,      0,    990,\n             0,   1941,  30709, 170714,  25386,      0,      0]), tensor([ 60604,      0,   3015,   3535,  36466,  48294,      0,      0,  23176,\n             0, 110788,  64014,  36260,    121,      0,      0]), tensor([  2545,      0,   7957,  13840,  36554,   2018,      0,      0,  75775,\n             0, 130539,  50957,  17807,    105,      0,      0]), tensor([52917,     0, 15839, 20268,   119, 64238,     0,     0,   121,     0,\n        90154,   105,   121,     0,     0,     0]), tensor([161089,      0,  24430,    134,  41126,  43802,      0,      0,    105,\n             0,   1561,      0,    105,      0,      0,      0]), tensor([  2598,      0, 171881,    977,   8318, 120919,      0,      0,      0,\n             0,   1201,      0,      0,      0,      0,      0]), tensor([  6843,      0,  72728, 116547,  14754,  38258,      0,      0,      0,\n             0,   9809,      0,      0,      0,      0,      0]), tensor([196346,      0,  97753, 116580, 115305,  41211,      0,      0,      0,\n             0,  13467,      0,      0,      0,      0,      0]), tensor([   105,      0,  54002,   4443, 175912,  22982,      0,      0,      0,\n             0,    105,      0,      0,      0,      0,      0]), tensor([    0,     0,  8849, 57399, 36554,   121,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([    0,     0, 24430, 32227, 64017,   121,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([     0,      0,  94598,  19362, 154592,    105,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0]), tensor([    0,     0, 10705,  8969,   121,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([    0,     0, 76348, 33757,   121,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([     0,      0,   6097, 133531, 149539,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0]), tensor([    0,     0, 24430,   105, 48455,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([   0,    0, 1013,    0, 6245,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0]), tensor([     0,      0, 189134,      0,  64747,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0]), tensor([    0,     0, 80750,     0,  4080,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([     0,      0,   8477,      0, 153407,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0]), tensor([    0,     0,   105,     0, 27294,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([   0,    0,    0,    0, 4080,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0]), tensor([    0,     0,     0,     0, 85004,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([    0,     0,     0,     0, 41300,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([    0,     0,     0,     0, 16023,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([   0,    0,    0,    0, 4455,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0]), tensor([    0,     0,     0,     0, 60423,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([     0,      0,      0,      0, 152228,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0]), tensor([   0,    0,    0,    0, 6421,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0]), tensor([  0,   0,   0,   0, 121,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]), tensor([  0,   0,   0,   0, 121,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]), tensor([  0,   0,   0,   0, 121,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]), tensor([    0,     0,     0,     0, 38369,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([     0,      0,      0,      0, 110097,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0]), tensor([    0,     0,     0,     0, 40851,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([    0,     0,     0,     0, 40188,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([   0,    0,    0,    0, 1796,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0]), tensor([  0,   0,   0,   0, 120,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]), tensor([    0,     0,     0,     0, 10899,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([   0,    0,    0,    0, 6198,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0]), tensor([   0,    0,    0,    0, 1201,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0]), tensor([    0,     0,     0,     0, 74953,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0]), tensor([  0,   0,   0,   0, 105,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])], [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]), tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])], tensor([1, 1, 1, 2, 1, 0, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2])]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 223\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu memory cleared\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 223\u001b[0m   \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[11], line 214\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheduler loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel trained\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    218\u001b[0m clear_gpu_memory()\n","Cell \u001b[0;32mIn[11], line 72\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, model, optimizer, scheduler, device, epochs)\u001b[0m\n\u001b[1;32m     69\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 72\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     74\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:961\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[0;32m--> 961\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"],"ename":"AttributeError","evalue":"'list' object has no attribute 'size'","output_type":"error"}]}]}