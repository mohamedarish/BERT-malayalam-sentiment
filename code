{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8223486,"sourceType":"datasetVersion","datasetId":4875499}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mohamedarish/with-dataset-as-input?scriptVersionId=175022245\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pyspark transformers torch tqdm scikit-learn sparknlp huggingface_hub fasttext","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T05:07:47.070995Z","iopub.execute_input":"2024-05-01T05:07:47.071536Z","iopub.status.idle":"2024-05-01T05:08:36.071368Z","shell.execute_reply.started":"2024-05-01T05:07:47.071509Z","shell.execute_reply":"2024-05-01T05:08:36.070444Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nCollecting sparknlp\n  Downloading sparknlp-1.0.0-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.22.2)\nRequirement already satisfied: fasttext in /opt/conda/lib/python3.10/site-packages (0.9.2)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nCollecting spark-nlp (from sparknlp)\n  Downloading spark_nlp-5.3.3-py2.py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext) (2.12.0)\nRequirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from fasttext) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\nDownloading spark_nlp-5.3.3-py2.py3-none-any.whl (568 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.4/568.4 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=1003fef026017eb9ac71c35455d698acbd027fd8f32ca2d38eae68d012908c7b\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: spark-nlp, sparknlp, pyspark\nSuccessfully installed pyspark-3.5.1 spark-nlp-5.3.3 sparknlp-1.0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LOGISTIC REGRESSION","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nfrom huggingface_hub import hf_hub_download\nimport fasttext\nimport fasttext.util\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nlogistic_regression_model = LogisticRegression(max_iter=1000)\n\n# Step 6: Train Model in Epochs\nepochs = 10\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    logistic_regression_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, logistic_regression_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, logistic_regression_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T07:22:43.197209Z","iopub.execute_input":"2024-04-29T07:22:43.197516Z","iopub.status.idle":"2024-04-29T07:23:05.571359Z","shell.execute_reply.started":"2024-04-29T07:22:43.197488Z","shell.execute_reply":"2024-04-29T07:23:05.568884Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:35<?, ?it/s]\n  0%|          | 0/7171 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.6739645795565472, Train Precision: 0.6817086427052726, Train Recall: 0.6739645795565472, Train F1-score: 0.6576976785981656\nValidation Accuracy: 0.637479085331846, Validation Precision: 0.6396109859134467, Validation Recall: 0.637479085331846, Validation F1-score: 0.617633474977311\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.6739645795565472, Train Precision: 0.6817086427052726, Train Recall: 0.6739645795565472, Train F1-score: 0.6576976785981656\nValidation Accuracy: 0.637479085331846, Validation Precision: 0.6396109859134467, Validation Recall: 0.637479085331846, Validation F1-score: 0.617633474977311\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.6739645795565472, Train Precision: 0.6817086427052726, Train Recall: 0.6739645795565472, Train F1-score: 0.6576976785981656\nValidation Accuracy: 0.637479085331846, Validation Precision: 0.6396109859134467, Validation Recall: 0.637479085331846, Validation F1-score: 0.617633474977311\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.6739645795565472, Train Precision: 0.6817086427052726, Train Recall: 0.6739645795565472, Train F1-score: 0.6576976785981656\nValidation Accuracy: 0.637479085331846, Validation Precision: 0.6396109859134467, Validation Recall: 0.637479085331846, Validation F1-score: 0.617633474977311\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.6739645795565472, Train Precision: 0.6817086427052726, Train Recall: 0.6739645795565472, Train F1-score: 0.6576976785981656\nValidation Accuracy: 0.637479085331846, Validation Precision: 0.6396109859134467, Validation Recall: 0.637479085331846, Validation F1-score: 0.617633474977311\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.6739645795565472, Train Precision: 0.6817086427052726, Train Recall: 0.6739645795565472, Train F1-score: 0.6576976785981656\nValidation Accuracy: 0.637479085331846, Validation Precision: 0.6396109859134467, Validation Recall: 0.637479085331846, Validation F1-score: 0.617633474977311\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.6739645795565472, Train Precision: 0.6817086427052726, Train Recall: 0.6739645795565472, Train F1-score: 0.6576976785981656\nValidation Accuracy: 0.637479085331846, Validation Precision: 0.6396109859134467, Validation Recall: 0.637479085331846, Validation F1-score: 0.617633474977311\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:00<?, ?it/s]","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m tqdm_bar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_train), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Create tqdm progress bar\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mlogistic_regression_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Calculate training metrics\u001b[39;00m\n\u001b[1;32m     64\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Logistic regression does not have a loss attribute\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1291\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1289\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1291\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:450\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    446\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C\n\u001b[1;32m    447\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[1;32m    448\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[1;32m    449\u001b[0m ]\n\u001b[0;32m--> 450\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    459\u001b[0m     solver,\n\u001b[1;32m    460\u001b[0m     opt_res,\n\u001b[1;32m    461\u001b[0m     max_iter,\n\u001b[1;32m    462\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    463\u001b[0m )\n\u001b[1;32m    464\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:365\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    359\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_optimize.py:77\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_optimize.py:71\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 71\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:278\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    276\u001b[0m     weights, intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_intercept(coef)\n\u001b[0;32m--> 278\u001b[0m loss, grad_pointwise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    285\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_penalty(weights, l2_reg_strength)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/_loss/loss.py:257\u001b[0m, in \u001b[0;36mBaseLoss.loss_gradient\u001b[0;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m ReadonlyArrayWrapper(sample_weight)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_gradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_prediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# RANDOM FOREST","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport fasttext.util\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nrandom_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Step 6: Train Model in Epochs\nepochs = 10\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    random_forest_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, random_forest_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, random_forest_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T07:23:05.572875Z","iopub.status.idle":"2024-04-29T07:23:05.573363Z","shell.execute_reply.started":"2024-04-29T07:23:05.573122Z","shell.execute_reply":"2024-04-29T07:23:05.573143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport fasttext\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nsvm_model = SVC(kernel='linear', random_state=42)\n\n# Step 6: Train Model in Epochs\nepochs = 10\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    svm_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, svm_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, svm_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T07:23:05.574593Z","iopub.status.idle":"2024-04-29T07:23:05.575042Z","shell.execute_reply.started":"2024-04-29T07:23:05.574801Z","shell.execute_reply":"2024-04-29T07:23:05.574818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BIDIRECTIONAL LSTM WITH ATTENTION LAYER","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Activation, Lambda, RepeatVector, Permute, Flatten\nimport tensorflow.keras.backend as K\nimport os\nimport torch\n\ndef clear_gpu_memory():\n  \"\"\"Frees memory allocated on the GPU.\"\"\"\n  if torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\"):\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n        dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\nprint(df.size)\n\ndf = df.dropna(subset=['clean_content'])\n\nprint(df.size)\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Preprocessing\nmaxlen = 10000  # Maximum sequence length\nmax_words = 200000  # Maximum number of words in vocabulary\n\n# Tokenize the text\ntokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(df['content'])\n\n# Convert text to sequences\nsequences = tokenizer.texts_to_sequences(df['content'])\n\n# Pad sequences to maxlen\nX = pad_sequences(sequences, maxlen=maxlen)\n\n# Label encoding for sentiments (assuming you have 'sentiment' column)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df['sentiment'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the Bidirectional LSTM with Attention Model\ninput_seq = Input(shape=(maxlen,))\nembedding = Embedding(max_words, 128, input_length=maxlen)(input_seq)\nlstm = Bidirectional(LSTM(64, return_sequences=True))(embedding)\n\n# Attention Mechanism\nattention = Dense(1, activation='tanh')(lstm)\nattention = Flatten()(attention)\nattention = Activation('softmax', name='attention_weights')(attention)\nattention = RepeatVector(128 * 2)(attention)\nattention = Permute([2, 1])(attention)\n\nsent_representation = Concatenate(axis=-1)([lstm, attention])\nsent_representation = Lambda(lambda x: K.sum(x, axis=1))(sent_representation)\n\noutput = Dense(1, activation='sigmoid')(sent_representation)\n\nmodel = Model(inputs=input_seq, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.1)\n\nclear_gpu_memory()\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy}')\n\n# Print training and validation metrics\nprint(\"Training Loss:\", history.history['loss'])\nprint(\"Training Accuracy:\", history.history['accuracy'])\nprint(\"Validation Loss:\", history.history['val_loss'])\nprint(\"Validation Accuracy:\", history.history['val_accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-29T07:23:05.578053Z","iopub.status.idle":"2024-04-29T07:23:05.578387Z","shell.execute_reply.started":"2024-04-29T07:23:05.578226Z","shell.execute_reply":"2024-04-29T07:23:05.57824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport os\nimport pandas as pd\nimport torch\n\ndef clear_gpu_memory():\n  \"\"\"Frees memory allocated on the GPU.\"\"\"\n  if torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Load Malayalam BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n# Define dataset class\nclass MalayalamDataset(Dataset):\n    def __init__(self, dataframe, max_len=128):\n        self.data = dataframe\n        self.max_len = max_len\n        self.texts = self.data.content.tolist()\n        self.targets = self.data.sentiment.tolist()\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = str(self.texts[index])\n        target = self.targets[index]\n\n        inputs = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'text': text,\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'target': torch.tensor(target, dtype=torch.long)\n        }\n\n# Load and preprocess dataset\ndef load_dataset(file_path):\n    df = pd.read_csv(file_path)\n    return df\n\n# Fine-tune BERT for sentiment analysis\ndef train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total_targets = 0\n        for batch in tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n            correct += (preds == targets.cpu().numpy()).sum()\n            total_targets += len(targets)\n\n            loss.backward()\n            optimizer.step()\n\n        avg_train_loss = total_loss / len(train_loader)\n        train_accuracy = correct / total_targets\n        print(f\"Average training loss for epoch {epoch+1}: {avg_train_loss}\")\n        print(f\"Training Accuracy for epoch {epoch+1}: {train_accuracy}\")\n\n        val_loss, val_acc, val_report = evaluate_model(model, val_loader, device)\n        print(f\"Validation loss: {val_loss}, Accuracy: {val_acc}\")\n        print(\"Validation Report:\")\n        print(val_report)\n\n        scheduler.step()\n\n# Evaluate the model\ndef evaluate_model(model, val_loader, device):\n    model.eval()\n    val_targets = []\n    val_outputs = []\n\n    total_val_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n\n            val_targets.extend(targets.cpu().numpy())\n            val_outputs.extend(preds)\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_targets, val_outputs)\n    val_report = classification_report(val_targets, val_outputs, target_names=[\"Negative\", \"Neutral\", \"Positive\"])\n    return avg_val_loss, val_accuracy, val_report\n# Main function to train and evaluate\ndef main():\n  tokenizer.save_pretrained(\"bert-tokenizer\")\n\n#   return\n\n  # # Directory containing your CSV files\n  directory = '/kaggle/input/malayalam-tweets/'\n\n  # # List to store DataFrames from each CSV file\n  dfs = []\n\n  # # Loop through each file in the directory\n  for filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n      file_path = os.path.join(directory, filename)\n      # Read the CSV file into a DataFrame\n      a_df = pd.read_csv(file_path)\n\n      if \"datetimee\" in a_df.columns:\n        # print(\"has datetimeee\")\n        a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n      # Append the DataFrame to the list\n      dfs.append(a_df)\n\n  # # Combine all DataFrames into a single DataFrame\n  df = pd.concat(dfs, ignore_index=True)\n\n  df = df.dropna(subset=['clean_content'])\n\n  df['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n  # Split dataset into train and validation\n  train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n\n  # Create datasets and dataloaders\n  train_dataset = MalayalamDataset(train_df)\n  val_dataset = MalayalamDataset(val_df)\n\n  batch_size = 16\n  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n  # Load pre-trained BERT model for sequence classification\n  model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)\n\n  # Send model to GPU, if available\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  model.to(device)\n\n  # Create optimizer\n  optimizer = AdamW(model.parameters(), lr=2e-5)\n\n  # Create scheduler\n  scheduler = ExponentialLR(optimizer, gamma=0.9)\n\n  # Train the model\n  train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=10)\n\n  clear_gpu_memory()\n\n  # Save the trained model\n  model.save_pretrained(\"malayalam_sentiment_model\")\n\nif __name__ == \"__main__\":\n  main()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-01T05:08:49.8163Z","iopub.execute_input":"2024-05-01T05:08:49.816627Z","iopub.status.idle":"2024-05-01T05:08:50.259154Z","shell.execute_reply.started":"2024-05-01T05:08:49.8166Z","shell.execute_reply":"2024-05-01T05:08:50.258169Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport os\nimport pandas as pd\nimport torch\n\ndef clear_gpu_memory():\n  \"\"\"Frees memory allocated on the GPU.\"\"\"\n  if torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Load Malayalam BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('l3cube-pune/malayalam-bert')\n\n# Define dataset class\nclass MalayalamDataset(Dataset):\n    def __init__(self, dataframe, max_len=128):\n        self.data = dataframe\n        self.max_len = max_len\n        self.texts = self.data.content.tolist()\n        self.targets = self.data.sentiment.tolist()\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = str(self.texts[index])\n        target = self.targets[index]\n\n        inputs = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'text': text,\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'target': torch.tensor(target, dtype=torch.long)\n        }\n\n# Load and preprocess dataset\ndef load_dataset(file_path):\n    df = pd.read_csv(file_path)\n    return df\n\n# Fine-tune BERT for sentiment analysis\ndef train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total_targets = 0\n        for batch in tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n            correct += (preds == targets.cpu().numpy()).sum()\n            total_targets += len(targets)\n\n            loss.backward()\n            optimizer.step()\n\n        avg_train_loss = total_loss / len(train_loader)\n        train_accuracy = correct / total_targets\n        print(f\"Average training loss for epoch {epoch+1}: {avg_train_loss}\")\n        print(f\"Training Accuracy for epoch {epoch+1}: {train_accuracy}\")\n\n        val_loss, val_acc, val_report = evaluate_model(model, val_loader, device)\n        print(f\"Validation loss: {val_loss}, Accuracy: {val_acc}\")\n        print(\"Validation Report:\")\n        print(val_report)\n\n        scheduler.step()\n\n# Evaluate the model\ndef evaluate_model(model, val_loader, device):\n    model.eval()\n    val_targets = []\n    val_outputs = []\n\n    total_val_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n\n            val_targets.extend(targets.cpu().numpy())\n            val_outputs.extend(preds)\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_targets, val_outputs)\n    val_report = classification_report(val_targets, val_outputs, target_names=[\"Negative\", \"Neutral\", \"Positive\"])\n    return avg_val_loss, val_accuracy, val_report\n# Main function to train and evaluate\ndef main():\n  tokenizer.save_pretrained(\"bert-tokenizer\")\n\n#   return\n  # # Directory containing your CSV files\n  directory = '/kaggle/input/malayalam-tweets/'\n\n  # # List to store DataFrames from each CSV file\n  dfs = []\n\n  # # Loop through each file in the directory\n  for filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n      file_path = os.path.join(directory, filename)\n      # Read the CSV file into a DataFrame\n      a_df = pd.read_csv(file_path)\n\n      if \"datetimee\" in a_df.columns:\n        # print(\"has datetimeee\")\n        a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n      # Append the DataFrame to the list\n      dfs.append(a_df)\n\n  # # Combine all DataFrames into a single DataFrame\n  df = pd.concat(dfs, ignore_index=True)\n\n  df = df.dropna(subset=['clean_content'])\n\n  df['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n  # Split dataset into train and validation\n  train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n\n  # Create datasets and dataloaders\n  train_dataset = MalayalamDataset(train_df)\n  val_dataset = MalayalamDataset(val_df)\n\n  batch_size = 16\n  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n  # Load pre-trained BERT model for sequence classification\n  model = BertForSequenceClassification.from_pretrained('l3cube-pune/malayalam-bert', num_labels=3)\n\n  # Send model to GPU, if available\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  model.to(device)\n\n  # Create optimizer\n  optimizer = AdamW(model.parameters(), lr=2e-5)\n\n  # Create scheduler\n  scheduler = ExponentialLR(optimizer, gamma=0.9)\n\n  # Train the model\n  train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=9)\n\n  clear_gpu_memory()\n\n  # Save the trained model\n  model.save_pretrained(\"malayalam_sentiment_model\")\n\nif __name__ == \"__main__\":\n  main()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-01T07:06:25.500379Z","iopub.execute_input":"2024-05-01T07:06:25.500921Z","iopub.status.idle":"2024-05-01T07:37:42.565705Z","shell.execute_reply.started":"2024-05-01T07:06:25.500888Z","shell.execute_reply":"2024-05-01T07:37:42.564848Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9c2232a298545fa8fc8e50e358ac8ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4564a558f1a4ca38ba4379203731ff8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d9bcc153e9744d7b6d57534e7bfc1d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ff299e329d1412b872fc6bb83025d83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"125ade4159344ebab6366118089acabe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/951M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c89d15ec75bb4bf8927230cf2d4ce38a"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at l3cube-pune/malayalam-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/9: 100%|██████████| 520/520 [03:20<00:00,  2.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 1: 0.8334679492964194\nTraining Accuracy for epoch 1: 0.6507879225309756\nValidation loss: 0.6741909456664118, Accuracy: 0.7067099567099567\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.54      0.79      0.64       149\n     Neutral       0.67      0.70      0.69       397\n    Positive       0.88      0.68      0.77       378\n\n    accuracy                           0.71       924\n   macro avg       0.70      0.72      0.70       924\nweighted avg       0.74      0.71      0.71       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/9: 100%|██████████| 520/520 [03:18<00:00,  2.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 2: 0.5040055001584384\nTraining Accuracy for epoch 2: 0.8011548177553229\nValidation loss: 0.5185192160565277, Accuracy: 0.7716450216450217\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.68      0.72      0.70       149\n     Neutral       0.77      0.70      0.73       397\n    Positive       0.81      0.87      0.84       378\n\n    accuracy                           0.77       924\n   macro avg       0.75      0.76      0.76       924\nweighted avg       0.77      0.77      0.77       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/9: 100%|██████████| 520/520 [03:18<00:00,  2.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 3: 0.30103175736104065\nTraining Accuracy for epoch 3: 0.9007578491519307\nValidation loss: 0.569028909478722, Accuracy: 0.762987012987013\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.68      0.68      0.68       149\n     Neutral       0.75      0.71      0.73       397\n    Positive       0.81      0.85      0.83       378\n\n    accuracy                           0.76       924\n   macro avg       0.74      0.75      0.75       924\nweighted avg       0.76      0.76      0.76       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/9: 100%|██████████| 520/520 [03:18<00:00,  2.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 4: 0.17391317379302704\nTraining Accuracy for epoch 4: 0.9506796583664141\nValidation loss: 0.7287946420496908, Accuracy: 0.7727272727272727\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.67      0.68      0.68       149\n     Neutral       0.74      0.76      0.75       397\n    Positive       0.85      0.82      0.83       378\n\n    accuracy                           0.77       924\n   macro avg       0.75      0.76      0.75       924\nweighted avg       0.77      0.77      0.77       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/9: 100%|██████████| 520/520 [03:18<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 5: 0.11596146999106098\nTraining Accuracy for epoch 5: 0.9704077950198484\nValidation loss: 0.7401616664539123, Accuracy: 0.7640692640692641\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.71      0.66      0.69       149\n     Neutral       0.74      0.72      0.73       397\n    Positive       0.80      0.85      0.83       378\n\n    accuracy                           0.76       924\n   macro avg       0.75      0.74      0.75       924\nweighted avg       0.76      0.76      0.76       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/9: 100%|██████████| 520/520 [03:18<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 6: 0.07799256583675743\nTraining Accuracy for epoch 6: 0.9823168531216168\nValidation loss: 0.8601820849781406, Accuracy: 0.7683982683982684\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.73      0.59      0.65       149\n     Neutral       0.72      0.78      0.75       397\n    Positive       0.84      0.83      0.83       378\n\n    accuracy                           0.77       924\n   macro avg       0.76      0.73      0.74       924\nweighted avg       0.77      0.77      0.77       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/9: 100%|██████████| 520/520 [03:17<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 7: 0.05464559578014394\nTraining Accuracy for epoch 7: 0.9885721159629496\nValidation loss: 0.9335898223879009, Accuracy: 0.7597402597402597\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.71      0.66      0.69       149\n     Neutral       0.69      0.82      0.75       397\n    Positive       0.88      0.73      0.80       378\n\n    accuracy                           0.76       924\n   macro avg       0.76      0.74      0.75       924\nweighted avg       0.77      0.76      0.76       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/9: 100%|██████████| 520/520 [03:18<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 8: 0.046070847681795174\nTraining Accuracy for epoch 8: 0.9906171057380008\nValidation loss: 0.9735342007258843, Accuracy: 0.762987012987013\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.74      0.58      0.65       149\n     Neutral       0.69      0.84      0.76       397\n    Positive       0.88      0.76      0.81       378\n\n    accuracy                           0.76       924\n   macro avg       0.77      0.72      0.74       924\nweighted avg       0.77      0.76      0.76       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/9: 100%|██████████| 520/520 [03:18<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 9: 0.035382372439982226\nTraining Accuracy for epoch 9: 0.9927823890292313\nValidation loss: 1.0158813428776017, Accuracy: 0.7575757575757576\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.70      0.60      0.64       149\n     Neutral       0.70      0.80      0.74       397\n    Positive       0.86      0.78      0.82       378\n\n    accuracy                           0.76       924\n   macro avg       0.75      0.72      0.74       924\nweighted avg       0.76      0.76      0.76       924\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport os\nimport pandas as pd\nimport torch\n\ndef clear_gpu_memory():\n  \"\"\"Frees memory allocated on the GPU.\"\"\"\n  if torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Load Malayalam BERT tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('RahulRaman/Malayalam-LM-RoBERTa')\n\n# Define dataset class\nclass MalayalamDataset(Dataset):\n    def __init__(self, dataframe, max_len=128):\n        self.data = dataframe\n        self.max_len = max_len\n        self.texts = self.data.content.tolist()\n        self.targets = self.data.sentiment.tolist()\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = str(self.texts[index])\n        target = self.targets[index]\n\n        inputs = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'text': text,\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'target': torch.tensor(target, dtype=torch.long)\n        }\n\n# Load and preprocess dataset\ndef load_dataset(file_path):\n    df = pd.read_csv(file_path)\n    return df\n\n# Fine-tune BERT for sentiment analysis\ndef train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total_targets = 0\n        for batch in tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n            correct += (preds == targets.cpu().numpy()).sum()\n            total_targets += len(targets)\n\n            loss.backward()\n            optimizer.step()\n\n        avg_train_loss = total_loss / len(train_loader)\n        train_accuracy = correct / total_targets\n        print(f\"Average training loss for epoch {epoch+1}: {avg_train_loss}\")\n        print(f\"Training Accuracy for epoch {epoch+1}: {train_accuracy}\")\n\n        val_loss, val_acc, val_report = evaluate_model(model, val_loader, device)\n        print(f\"Validation loss: {val_loss}, Accuracy: {val_acc}\")\n        print(\"Validation Report:\")\n        print(val_report)\n\n        scheduler.step()\n\n# Evaluate the model\ndef evaluate_model(model, val_loader, device):\n    model.eval()\n    val_targets = []\n    val_outputs = []\n\n    total_val_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n\n            val_targets.extend(targets.cpu().numpy())\n            val_outputs.extend(preds)\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_targets, val_outputs)\n    val_report = classification_report(val_targets, val_outputs, target_names=[\"Negative\", \"Neutral\", \"Positive\"])\n    return avg_val_loss, val_accuracy, val_report\n# Main function to train and evaluate\ndef main():\n  tokenizer.save_pretrained(\"bert-tokenizer\")\n\n#   return\n  # # Directory containing your CSV files\n  directory = '/kaggle/input/malayalam-tweets/'\n\n  # # List to store DataFrames from each CSV file\n  dfs = []\n\n  # # Loop through each file in the directory\n  for filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n      file_path = os.path.join(directory, filename)\n      # Read the CSV file into a DataFrame\n      a_df = pd.read_csv(file_path)\n\n      if \"datetimee\" in a_df.columns:\n        # print(\"has datetimeee\")\n        a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n      # Append the DataFrame to the list\n      dfs.append(a_df)\n\n  # # Combine all DataFrames into a single DataFrame\n  df = pd.concat(dfs, ignore_index=True)\n\n  df = df.dropna(subset=['clean_content'])\n\n  df['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n  # Split dataset into train and validation\n  train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n\n  # Create datasets and dataloaders\n  train_dataset = MalayalamDataset(train_df)\n  val_dataset = MalayalamDataset(val_df)\n\n  batch_size = 16\n  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n  # Load pre-trained BERT model for sequence classification\n  model = RobertaForSequenceClassification.from_pretrained('RahulRaman/Malayalam-LM-RoBERTa', num_labels=7)\n\n  # Send model to GPU, if available\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  model.to(device)\n\n  # Create optimizer\n  optimizer = AdamW(model.parameters(), lr=2e-5)\n\n  # Create scheduler\n  scheduler = ExponentialLR(optimizer, gamma=0.9)\n\n  # Train the model\n  train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=6)\n\n  clear_gpu_memory()\n\n  # Save the trained model\n  model.save_pretrained(\"malayalam_sentiment_model\")\n\nif __name__ == \"__main__\":\n  main()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-01T06:54:20.673649Z","iopub.execute_input":"2024-05-01T06:54:20.674155Z","iopub.status.idle":"2024-05-01T07:06:19.205357Z","shell.execute_reply.started":"2024-05-01T06:54:20.674117Z","shell.execute_reply":"2024-05-01T07:06:19.204018Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/728k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0331e130d4074c7095739e058c132f35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/513k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10a16191b5eb4b7bbc3c550554c77312"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/708 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac0634b7300e44cbb80ff5b58a9e1075"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/273M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c2bd2e5da9a4427b76ba8447c0b6909"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at RahulRaman/Malayalam-LM-RoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/10: 100%|██████████| 520/520 [01:26<00:00,  6.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 1: 0.9113095859495494\nTraining Accuracy for epoch 1: 0.5694695055936485\nValidation loss: 0.8002225474036974, Accuracy: 0.6212121212121212\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.41      0.43      0.42       149\n     Neutral       0.66      0.51      0.58       397\n    Positive       0.67      0.81      0.73       378\n\n    accuracy                           0.62       924\n   macro avg       0.58      0.58      0.58       924\nweighted avg       0.62      0.62      0.61       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 520/520 [01:28<00:00,  5.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 2: 0.702523318792765\nTraining Accuracy for epoch 2: 0.6861542162877421\nValidation loss: 0.7496802503692692, Accuracy: 0.6536796536796536\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.47      0.36      0.41       149\n     Neutral       0.61      0.73      0.66       397\n    Positive       0.79      0.69      0.73       378\n\n    accuracy                           0.65       924\n   macro avg       0.62      0.59      0.60       924\nweighted avg       0.66      0.65      0.65       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 520/520 [01:28<00:00,  5.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 3: 0.435906805169697\nTraining Accuracy for epoch 3: 0.8289426199927824\nValidation loss: 0.859563747870511, Accuracy: 0.6525974025974026\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.44      0.51      0.47       149\n     Neutral       0.67      0.57      0.61       397\n    Positive       0.73      0.80      0.76       378\n\n    accuracy                           0.65       924\n   macro avg       0.61      0.63      0.62       924\nweighted avg       0.66      0.65      0.65       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 520/520 [01:28<00:00,  5.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 4: 0.1363291963773708\nTraining Accuracy for epoch 4: 0.9578972693371828\nValidation loss: 1.2854431194478069, Accuracy: 0.6461038961038961\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.46      0.34      0.39       149\n     Neutral       0.65      0.62      0.63       397\n    Positive       0.69      0.79      0.74       378\n\n    accuracy                           0.65       924\n   macro avg       0.60      0.59      0.59       924\nweighted avg       0.64      0.65      0.64       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 520/520 [01:28<00:00,  5.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 5: 0.03447052451217762\nTraining Accuracy for epoch 5: 0.9909779862865391\nValidation loss: 1.5608019253303265, Accuracy: 0.6352813852813853\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.41      0.54      0.47       149\n     Neutral       0.69      0.52      0.59       397\n    Positive       0.70      0.80      0.75       378\n\n    accuracy                           0.64       924\n   macro avg       0.60      0.62      0.60       924\nweighted avg       0.65      0.64      0.63       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 520/520 [01:28<00:00,  5.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 6: 0.010315783748340506\nTraining Accuracy for epoch 6: 0.9986767713220257\nValidation loss: 1.6579756700787052, Accuracy: 0.6536796536796536\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.45      0.47      0.46       149\n     Neutral       0.65      0.63      0.64       397\n    Positive       0.75      0.75      0.75       378\n\n    accuracy                           0.65       924\n   macro avg       0.61      0.62      0.61       924\nweighted avg       0.66      0.65      0.65       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 520/520 [01:28<00:00,  5.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 7: 0.004184326890390366\nTraining Accuracy for epoch 7: 0.9992782389029231\nValidation loss: 1.7631581240686878, Accuracy: 0.6439393939393939\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.41      0.53      0.46       149\n     Neutral       0.66      0.58      0.62       397\n    Positive       0.74      0.75      0.75       378\n\n    accuracy                           0.64       924\n   macro avg       0.61      0.62      0.61       924\nweighted avg       0.65      0.64      0.65       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10:  73%|███████▎  | 382/520 [01:05<00:23,  5.85it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 188\u001b[0m\n\u001b[1;32m    185\u001b[0m   model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmalayalam_sentiment_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 188\u001b[0m   \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[2], line 180\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ExponentialLR(optimizer, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m clear_gpu_memory()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 82\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, model, optimizer, scheduler, device, epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m targets\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     80\u001b[0m     total_targets \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(targets)\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     85\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"This would've been used for getting files from gDrive but now it's a kaggle dataset\n!gdown --folder https://drive.google.com/drive/folders/1_-3L_E3MxkijO8rLd-fsPCYZv5bZFh2m?usp=sharing","metadata":{"execution":{"iopub.status.busy":"2024-04-25T06:32:50.670697Z","iopub.execute_input":"2024-04-25T06:32:50.672693Z","iopub.status.idle":"2024-04-25T06:33:20.476448Z","shell.execute_reply.started":"2024-04-25T06:32:50.672644Z","shell.execute_reply":"2024-04-25T06:33:20.475153Z"}}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, DoubleType\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nimport os\nimport pandas as pd\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nimport numpy as np\nimport fasttext\n\n# Initialize SparkSession\nspark = SparkSession.builder \\\n    .appName(\"Sentiment Analysis with FastText\") \\\n    .config(\"spark.executor.memory\", \"6g\") \\\n    .config(\"spark.driver.memory\", \"4g\") \\\n    .config(\"spark.executor.cores\", \"2\") \\\n    .config(\"spark.executor.instances\", \"4\") \\\n    .config(\"spark.executor.memoryOverhead\", \"1g\") \\\n    .config(\"spark.shuffle.memoryFraction\", \"0.4\") \\\n    .config(\"spark.broadcast.blockSize\", \"512m\") \\\n    .getOrCreate()\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# Precompute document vectors\ndocument_vectors = []\nfor text in df[\"clean_content\"]:\n    vector = model.get_sentence_vector(text)\n    print(text)\n    vector_list = vector.tolist()\n    document_vectors.append(vector_list)\n\n# Add document vectors to DataFrame\ndf[\"document_vector\"] = document_vectors\n\nprint(f'model cleared {df.shape}')\n\nembedding_dim = len(model.words)  # Dimension of the fastText word vectors\n\nvocab_size = len(model.words)\n\nmodel = None\n\n# Load data\ndata = spark.createDataFrame(df)\n\nprint(\"loaded dataset\")\n\nbatch_size = 32\n\n# Download and load FastText model\n\n# def generate_doc_vector(text):\n#     vector = model.get_sentence_vector(text)\n#     print(vector)\n#     return Vectors.dense(vector)\n\n# print(model_path)\n\n# broadcast_model = spark.sparkContext.broadcast(model)\n\n# # Function to generate document vectors using FastText model\ndef generate_doc_vector(text):\n#     model = broadcast_model.value\n#     vector = model.get_sentence_vector(text)\n    return Vectors.dense(text)\n\n# Register UDF\ngenerate_doc_vector_udf = udf(generate_doc_vector, VectorUDT())\n\n# Generate document vectors for Malayalam text data\ndata = data.withColumn(\"vector\", generate_doc_vector_udf(\"document_vector\"))\n\nprint(\"vectorized\")\n\n# Convert sentiment labels to numeric indices\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\ndata = indexer.fit(data).transform(data)\n\nprint(\"Stirign indexed labels\")\n\n# # Assemble features into a vector\n# assembler = VectorAssembler(inputCols=[\"vector\"], outputCol=\"features\")\n# data = assembler.transform(data)\n\n# Split data into training and testing sets\ntrain_data, test_data = data.randomSplit([0.8, 0.2], seed=123)\n\nprint(\"train text split done\")\n\n# # Define logistic regression model\n# lr = LogisticRegression(featuresCol=\"vector\", labelCol=\"label\")\n\n# print(\"spark mem bad\")\n\n# # sample_data = train_data.sample(fraction=0.5, seed=123)\n# # Specify number of epochs\n# num_epochs = 10\n\n# for epoch in range(num_epochs):\n#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n#     # Train the model\n#     lr_model = lr.fit(train_data)\n    \n#     print(\"WooHoo YAY WOW MEME BIG BOY COWABANGA THIS IS AWESOME\")\n\n#     # Make predictions on training data\n#     train_predictions = lr_model.transform(train_data)\n    \n#     print(\"MODEL TRANSFORMATION DONE YOOHOO WOOHOO FORTNITE MOUNTAIN DEW AND DORITOS MON+M LET ME STAY UP LATE TO PLAY FORTNITE\")\n    \n\n#     # Evaluate model performance on training data\n#     train_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n#     train_accuracy = train_evaluator.evaluate(train_predictions)\n#     print(f\"Training Accuracy: {train_accuracy}\")\n\n#     # Make predictions on testing data\n#     test_predictions = lr_model.transform(test_data)\n\n#     # Evaluate model performance on testing data\n#     test_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n#     test_accuracy = test_evaluator.evaluate(test_predictions)\n#     print(f\"Testing Accuracy: {test_accuracy}\")\n\ntrain_df_pandas = train_data.toPandas()\ntest_df_pandas = test_data.toPandas()\n\nprint(\"Converted to pandas\")\n\n# Convert text data to numerical sequences (e.g., using tokenization and padding)\n# Preprocess labels (e.g., convert to numerical format if necessary)\n\n# TensorFlow model building and training\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Attention\nfrom tensorflow.keras.models import Model\n\nprint(\"Imports done\")\n\n# Split the data into features (document vectors) and labels (sentiment)\nX_train = train_df_pandas[\"document_vector\"].values\ny_train = train_df_pandas[\"sentiment\"].values\n\nX_test = test_df_pandas[\"document_vector\"].values\ny_test = test_df_pandas[\"sentiment\"].values\n\nprint(\"Split inott x and Y\")\n\n# Define the maximum sequence length and other parameters\nmax_sequence_length = 8092  # Example value, adjust based on your data\nlstm_units = 64  # Example value, adjust based on your requirements\nnum_classes = 3  # Number of sentiment classes (0, 1, 2)\n\n# Perform the train-test split using sklearn\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\nprint(\"More split?\")\n\n# Convert the data to numpy arrays\nX_train = np.array(X_train.tolist())\nX_test = np.array(X_test.tolist())\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nprint(\"FRom numpy to array done\")\n\n# Define model architecture\ninput_layer = Input(shape=(max_sequence_length,))\nembedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\nlstm_layer = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(embedding_layer)\nattention_layer = Attention()([lstm_layer, lstm_layer])\noutput_layer = Dense(num_classes, activation='softmax')(attention_layer)\n\nprint(\"LSTM DEFINED\")\n\nmodel = Model(inputs=input_layer, outputs=output_layer)\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nprint(\"MODEL COMPILED\")\n\n# Train the model\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=10)\n\n\nprint(\"MODEL FITTED\")\n\n# Evaluate the model\nloss, accuracy = model.evaluate(test_sequences, test_labels)\nprint(\"Test Loss:\", loss)\nprint(\"Test Accuracy:\", accuracy)\n\n# Stop SparkSession\nspark.stop()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-29T07:23:05.585919Z","iopub.status.idle":"2024-04-29T07:23:05.586229Z","shell.execute_reply.started":"2024-04-29T07:23:05.586072Z","shell.execute_reply":"2024-04-29T07:23:05.586085Z"},"trusted":true},"execution_count":null,"outputs":[]}]}