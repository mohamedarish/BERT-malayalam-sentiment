{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8223486,"sourceType":"datasetVersion","datasetId":4875499}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark transformers torch tqdm scikit-learn sparknlp huggingface_hub fasttext","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-05T07:14:58.918639Z","iopub.execute_input":"2024-05-05T07:14:58.919368Z","iopub.status.idle":"2024-05-05T07:15:46.251679Z","shell.execute_reply.started":"2024-05-05T07:14:58.919335Z","shell.execute_reply":"2024-05-05T07:15:46.250556Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nCollecting sparknlp\n  Downloading sparknlp-1.0.0-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.22.2)\nRequirement already satisfied: fasttext in /opt/conda/lib/python3.10/site-packages (0.9.2)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nCollecting spark-nlp (from sparknlp)\n  Downloading spark_nlp-5.3.3-py2.py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext) (2.12.0)\nRequirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from fasttext) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading sparknlp-1.0.0-py3-none-any.whl (1.4 kB)\nDownloading spark_nlp-5.3.3-py2.py3-none-any.whl (568 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.4/568.4 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=4222db7ed49a4b26b63c0d7f258f939f0948b313f68fc5c32e5cd688ebf26acf\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: spark-nlp, sparknlp, pyspark\nSuccessfully installed pyspark-3.5.1 spark-nlp-5.3.3 sparknlp-1.0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LOGISTIC REGRESSION","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nfrom huggingface_hub import hf_hub_download\nimport fasttext\nimport fasttext.util\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nlogistic_regression_model = LogisticRegression(max_iter=1000)\n\n# Step 6: Train Model in Epochs\nepochs = 1\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    logistic_regression_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, logistic_regression_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, logistic_regression_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T06:30:18.861532Z","iopub.execute_input":"2024-05-04T06:30:18.861933Z","iopub.status.idle":"2024-05-04T06:30:42.093472Z","shell.execute_reply.started":"2024-05-04T06:30:18.861903Z","shell.execute_reply":"2024-05-04T06:30:42.088598Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.6739645795565472, Train Precision: 0.6817086427052726, Train Recall: 0.6739645795565472, Train F1-score: 0.6576976785981656\nValidation Accuracy: 0.637479085331846, Validation Precision: 0.6396109859134467, Validation Recall: 0.637479085331846, Validation F1-score: 0.617633474977311\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RANDOM FOREST","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport fasttext.util\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nrandom_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Step 6: Train Model in Epochs\nepochs = 1\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    random_forest_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, random_forest_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, random_forest_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T06:30:58.140088Z","iopub.execute_input":"2024-05-04T06:30:58.141047Z","iopub.status.idle":"2024-05-04T06:32:27.113059Z","shell.execute_reply.started":"2024-05-04T06:30:58.141012Z","shell.execute_reply":"2024-05-04T06:32:27.112162Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [00:11<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.9995816483056756, Train Precision: 0.9995819474761473, Train Recall: 0.9995816483056756, Train F1-score: 0.999581616854672\nValidation Accuracy: 0.6296709425543782, Validation Precision: 0.6450759085951593, Validation Recall: 0.6296709425543782, Validation F1-score: 0.5991409452797823\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport fasttext\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nsvm_model = SVC(kernel='linear', random_state=42)\n\n# Step 6: Train Model in Epochs\nepochs = 1\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    svm_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, svm_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, svm_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T06:34:21.520298Z","iopub.execute_input":"2024-05-04T06:34:21.520942Z","iopub.status.idle":"2024-05-04T06:35:36.518176Z","shell.execute_reply.started":"2024-05-04T06:34:21.520908Z","shell.execute_reply":"2024-05-04T06:35:36.517263Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/7171 [01:00<?, ?it/s]\n  0%|          | 0/7171 [00:31<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: None, Train Accuracy: 0.6584855668665458, Train Precision: 0.6789439172896629, Train Recall: 0.6584855668665458, Train F1-score: 0.6297087537970312\nValidation Accuracy: 0.6291132180702733, Validation Precision: 0.6349227133713751, Validation Recall: 0.6291132180702733, Validation F1-score: 0.5983744133620498\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BIDIRECTIONAL LSTM WITH ATTENTION LAYER","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Activation, Lambda, RepeatVector, Permute, Flatten\nimport tensorflow.keras.backend as K\nimport os\nimport torch\n\ndef clear_gpu_memory():\n  \"\"\"Frees memory allocated on the GPU.\"\"\"\n  if torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\"):\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n        dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\n# print(df.size)\n\ndf = df.dropna(subset=['clean_content'])\n\nprint(df.size)\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Preprocessing\nmaxlen = 10000  # Maximum sequence length\nmax_words = 200000  # Maximum number of words in vocabulary\n\n# Tokenize the text\ntokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(df['content'])\n\n# Convert text to sequences\nsequences = tokenizer.texts_to_sequences(df['content'])\n\n# Pad sequences to maxlen\nX = pad_sequences(sequences, maxlen=maxlen)\n\n# Label encoding for sentiments (assuming you have 'sentiment' column)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df['sentiment'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the Bidirectional LSTM with Attention Model\ninput_seq = Input(shape=(maxlen,))\nembedding = Embedding(max_words, 128, input_length=maxlen)(input_seq)\nlstm = Bidirectional(LSTM(64, return_sequences=True))(embedding)\n\n# Attention Mechanism\nattention = Dense(1, activation='tanh')(lstm)\nattention = Flatten()(attention)\nattention = Activation('softmax', name='attention_weights')(attention)\nattention = RepeatVector(128 * 2)(attention)\nattention = Permute([2, 1])(attention)\n\nsent_representation = Concatenate(axis=-1)([lstm, attention])\nsent_representation = Lambda(lambda x: K.sum(x, axis=1))(sent_representation)\n\noutput = Dense(1, activation='sigmoid')(sent_representation)\n\nmodel = Model(inputs=input_seq, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\nclear_gpu_memory()\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy}')\n\n# Print training and validation metrics\nprint(\"Training Loss:\", history.history['loss'])\nprint(\"Training Accuracy:\", history.history['accuracy'])\nprint(\"Validation Loss:\", history.history['val_loss'])\nprint(\"Validation Accuracy:\", history.history['val_accuracy'])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-04T06:36:20.065373Z","iopub.execute_input":"2024-05-04T06:36:20.066232Z","iopub.status.idle":"2024-05-04T06:56:53.549350Z","shell.execute_reply.started":"2024-05-04T06:36:20.066198Z","shell.execute_reply":"2024-05-04T06:56:53.548322Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"64680\n64659\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 573ms/step - accuracy: 0.4114 - loss: -25902.7051 - val_accuracy: 0.4154 - val_loss: -68951.6797\nEpoch 2/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 576ms/step - accuracy: 0.4128 - loss: -81250.4688 - val_accuracy: 0.4154 - val_loss: -120757.0312\nEpoch 3/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 578ms/step - accuracy: 0.4089 - loss: -157105.2656 - val_accuracy: 0.4154 - val_loss: -192876.8438\nEpoch 4/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 577ms/step - accuracy: 0.4001 - loss: -311555.7812 - val_accuracy: 0.4154 - val_loss: -268831.4688\nEpoch 5/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 577ms/step - accuracy: 0.4020 - loss: -418399.0312 - val_accuracy: 0.4154 - val_loss: -341662.7812\nEpoch 6/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 577ms/step - accuracy: 0.4063 - loss: -512298.0938 - val_accuracy: 0.4154 - val_loss: -398410.5938\nEpoch 7/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 577ms/step - accuracy: 0.4089 - loss: -618858.2500 - val_accuracy: 0.4154 - val_loss: -481179.6562\nEpoch 8/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 576ms/step - accuracy: 0.4114 - loss: -716554.6250 - val_accuracy: 0.4154 - val_loss: -537410.1875\nEpoch 9/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 576ms/step - accuracy: 0.4170 - loss: -798325.9375 - val_accuracy: 0.4154 - val_loss: -606501.5625\nEpoch 10/10\n\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 576ms/step - accuracy: 0.4174 - loss: -877242.3125 - val_accuracy: 0.4154 - val_loss: -654121.8125\n\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 228ms/step - accuracy: 0.4289 - loss: -728310.8125\nTest Accuracy: 0.4339826703071594\nTraining Loss: [-41889.4453125, -93974.625, -196679.625, -332648.25, -444626.625, -545343.3125, -642447.375, -742854.3125, -838400.625, -931623.75]\nTraining Accuracy: [0.4129323363304138, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636, 0.41338345408439636]\nValidation Loss: [-68951.6796875, -120757.03125, -192876.84375, -268831.46875, -341662.78125, -398410.59375, -481179.65625, -537410.1875, -606501.5625, -654121.8125]\nValidation Accuracy: [0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094, 0.41542625427246094]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport os\nimport pandas as pd\nimport torch\n\ndef clear_gpu_memory():\n    \"\"\"Frees memory allocated on the GPU.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n# Load Malayalam BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('l3cube-pune/malayalam-bert')\n\n# Define dataset class\nclass MalayalamDataset(Dataset):\n    def __init__(self, dataframe, max_len=128):\n        self.data = dataframe\n        self.max_len = max_len\n        self.texts = self.data.content.tolist()\n        self.targets = self.data.sentiment.tolist()\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = str(self.texts[index])\n        target = self.targets[index]\n\n        inputs = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'text': text,\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'target': torch.tensor(target, dtype=torch.long)\n        }\n\n# Load and preprocess dataset\ndef load_dataset(file_path):\n    df = pd.read_csv(file_path)\n    return df\n\ndef create_malayalam_dataset(content_col, sentiment_col, max_len=128):\n    tokenizer = BertTokenizer.from_pretrained('l3cube-pune/malayalam-bert')\n\n    def tokenize_text(content, sentiment):\n        inputs = tokenizer.encode_plus(\n            content,\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            return_token_type_ids=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'text': content,\n            'input_ids': inputs['input_ids'].flatten().tolist(),\n            'attention_mask': inputs['attention_mask'].flatten().tolist(),\n            'target': int(sentiment)\n        }\n\n    return udf(tokenize_text, StructType([\n        StructField('text', StringType(), True),\n        StructField('input_ids', ArrayType(IntegerType()), True),\n        StructField('attention_mask', ArrayType(IntegerType()), True),\n        StructField('target', IntegerType(), True)\n    ]))\n\n# Fine-tune BERT for sentiment analysis\ndef train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total_targets = 0\n        for batch in tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n            correct += (preds == targets.cpu().numpy()).sum()\n            total_targets += len(targets)\n\n            loss.backward()\n            optimizer.step()\n\n        avg_train_loss = total_loss / len(train_loader)\n        train_accuracy = correct / total_targets\n        print(f\"Average training loss for epoch {epoch+1}: {avg_train_loss}\")\n        print(f\"Training Accuracy for epoch {epoch+1}: {train_accuracy}\")\n\n        val_loss, val_acc, val_report = evaluate_model(model, val_loader, device)\n        print(f\"Validation loss: {val_loss}, Accuracy: {val_acc}\")\n        print(\"Validation Report:\")\n        print(val_report)\n\n        scheduler.step()\n\n# Evaluate the model\ndef evaluate_model(model, val_loader, device):\n    model.eval()\n    val_targets = []\n    val_outputs = []\n\n    total_val_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n\n            val_targets.extend(targets.cpu().numpy())\n            val_outputs.extend(preds)\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_targets, val_outputs)\n    val_report = classification_report(val_targets, val_outputs, target_names=[\"Negative\", \"Neutral\", \"Positive\"])\n    return avg_val_loss, val_accuracy, val_report\n\n# Main function to train and evaluate\ndef main():\n    tokenizer.save_pretrained(\"bert-tokenizer\")\n\n    # Create a SparkSession\n    spark = SparkSession.builder \\\n        .appName(\"Malayalam Tweets Analysis\") \\\n        .getOrCreate()\n\n    #   return\n    # # Directory containing your CSV files\n    directory = '/kaggle/input/malayalam-tweets/'\n\n    # # List to store DataFrames from each CSV file\n    dfs = []\n\n    # # Loop through each file in the directory\n    for filename in os.listdir(directory):\n        if filename.endswith(\".csv\") and \"total\" not in filename:\n            file_path = os.path.join(directory, filename)\n            # Read the CSV file into a DataFrame\n            a_df = pd.read_csv(file_path)\n\n            if \"datetimee\" in a_df.columns:\n                # print(\"has datetimeee\")\n                a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n    # # Combine all DataFrames into a single DataFrame\n    df = pd.concat(dfs, ignore_index=True)\n\n    df = df.dropna(subset=['clean_content'])\n\n    df['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n    \n    df_spark = spark.createDataFrame(df)\n\n    # Define UDF to create Malayalam Dataset\n    create_dataset_udf = create_malayalam_dataset(\"clean_content\", \"sentiment\")\n\n    # Apply UDF to create train_dataset and val_dataset\n    df_spark = df_spark.withColumn(\"malayalam_dataset\", create_dataset_udf(df_spark[\"clean_content\"], df_spark[\"sentiment\"]))\n    \n    # Split dataset into train and validation\n    train_df, val_df = df_spark.randomSplit([0.9, 0.1], seed=42)\n\n    # Convert Spark DataFrame to pandas DataFrame (optional)\n    train_df_pandas = train_df.select(\"malayalam_dataset\").toPandas()\n    val_df_pandas = val_df.select(\"malayalam_dataset\").toPandas()\n\n    # Convert pandas DataFrames to datasets\n    train_dataset = [row.malayalam_dataset for _, row in train_df_pandas.iterrows()]\n    val_dataset = [row.malayalam_dataset for _, row in val_df_pandas.iterrows()]\n    \n    # Split dataset into train and validation\n    # train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n\n    # Create datasets and dataloaders\n    # train_dataset = MalayalamDataset(train_df)\n    # val_dataset = MalayalamDataset(val_df)\n\n    batch_size = 16\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    # Load pre-trained BERT model for sequence classification\n    model = BertForSequenceClassification.from_pretrained('l3cube-pune/malayalam-bert', num_labels=3)\n\n    # Send model to GPU, if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Create optimizer\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n\n    # Create scheduler\n    scheduler = ExponentialLR(optimizer, gamma=0.9)\n\n    # Train the model\n    train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=9)\n\n    clear_gpu_memory()\n\n    # Save the trained model\n    model.save_pretrained(\"malayalam_sentiment_model\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T07:16:13.838953Z","iopub.execute_input":"2024-05-05T07:16:13.839347Z","iopub.status.idle":"2024-05-05T07:48:10.351882Z","shell.execute_reply.started":"2024-05-05T07:16:13.839309Z","shell.execute_reply":"2024-05-05T07:48:10.351103Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76d90637b64f48a58f139edeaad18ac6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"141bd633bf3c43cdbdd836b924299ea3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d33900d1e038468c93da1b8d3cab024b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ef0c8761e9d4355a6249bc8a236650d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"748b3c5c8c474849be12ff50b4926e4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/951M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9de827f0b9ee430d9e9ebf7f68d679da"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at l3cube-pune/malayalam-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/9: 100%|██████████| 520/520 [03:20<00:00,  2.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 1: 0.8310751753930862\nTraining Accuracy for epoch 1: 0.6555996631781547\nValidation loss: 0.693876336874633, Accuracy: 0.7132034632034632\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.75      0.38      0.51       149\n     Neutral       0.71      0.62      0.66       397\n    Positive       0.71      0.94      0.81       378\n\n    accuracy                           0.71       924\n   macro avg       0.72      0.65      0.66       924\nweighted avg       0.72      0.71      0.70       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/9: 100%|██████████| 520/520 [03:23<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 2: 0.5039584225473496\nTraining Accuracy for epoch 2: 0.8046433297245279\nValidation loss: 0.5500391087141531, Accuracy: 0.762987012987013\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.65      0.68      0.67       149\n     Neutral       0.74      0.75      0.74       397\n    Positive       0.84      0.81      0.82       378\n\n    accuracy                           0.76       924\n   macro avg       0.74      0.75      0.74       924\nweighted avg       0.77      0.76      0.76       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/9: 100%|██████████| 520/520 [03:23<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 3: 0.292569544682136\nTraining Accuracy for epoch 3: 0.9042463611211355\nValidation loss: 0.597180958066521, Accuracy: 0.7727272727272727\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.76      0.66      0.71       149\n     Neutral       0.74      0.75      0.74       397\n    Positive       0.81      0.84      0.83       378\n\n    accuracy                           0.77       924\n   macro avg       0.77      0.75      0.76       924\nweighted avg       0.77      0.77      0.77       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/9: 100%|██████████| 520/520 [03:23<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 4: 0.1573878629777867\nTraining Accuracy for epoch 4: 0.958739323950439\nValidation loss: 0.6981892177257044, Accuracy: 0.7683982683982684\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.71      0.68      0.70       149\n     Neutral       0.72      0.78      0.75       397\n    Positive       0.85      0.79      0.82       378\n\n    accuracy                           0.77       924\n   macro avg       0.76      0.75      0.76       924\nweighted avg       0.77      0.77      0.77       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/9: 100%|██████████| 520/520 [03:23<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 5: 0.09801629768469586\nTraining Accuracy for epoch 5: 0.9770239384097197\nValidation loss: 0.7760323874074323, Accuracy: 0.7857142857142857\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.74      0.68      0.71       149\n     Neutral       0.76      0.76      0.76       397\n    Positive       0.83      0.85      0.84       378\n\n    accuracy                           0.79       924\n   macro avg       0.78      0.77      0.77       924\nweighted avg       0.78      0.79      0.79       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/9: 100%|██████████| 520/520 [03:23<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 6: 0.07370285351509945\nTraining Accuracy for epoch 6: 0.9829183207025142\nValidation loss: 0.8403855656948069, Accuracy: 0.7640692640692641\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.64      0.80      0.71       149\n     Neutral       0.74      0.72      0.73       397\n    Positive       0.86      0.80      0.83       378\n\n    accuracy                           0.76       924\n   macro avg       0.75      0.77      0.76       924\nweighted avg       0.77      0.76      0.77       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/9: 100%|██████████| 520/520 [03:23<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 7: 0.0568852247374777\nTraining Accuracy for epoch 7: 0.9878503548658727\nValidation loss: 0.8888104273455923, Accuracy: 0.7619047619047619\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.72      0.64      0.68       149\n     Neutral       0.71      0.77      0.74       397\n    Positive       0.84      0.80      0.82       378\n\n    accuracy                           0.76       924\n   macro avg       0.76      0.74      0.75       924\nweighted avg       0.76      0.76      0.76       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/9: 100%|██████████| 520/520 [03:23<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 8: 0.04368877311040146\nTraining Accuracy for epoch 8: 0.9909779862865391\nValidation loss: 0.930901533185408, Accuracy: 0.7738095238095238\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.66      0.81      0.72       149\n     Neutral       0.77      0.70      0.73       397\n    Positive       0.84      0.84      0.84       378\n\n    accuracy                           0.77       924\n   macro avg       0.75      0.78      0.76       924\nweighted avg       0.78      0.77      0.77       924\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/9: 100%|██████████| 520/520 [03:23<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average training loss for epoch 9: 0.02881202327553183\nTraining Accuracy for epoch 9: 0.9941056177072056\nValidation loss: 0.9129899114627262, Accuracy: 0.7835497835497836\nValidation Report:\n              precision    recall  f1-score   support\n\n    Negative       0.72      0.70      0.71       149\n     Neutral       0.75      0.76      0.76       397\n    Positive       0.84      0.84      0.84       378\n\n    accuracy                           0.78       924\n   macro avg       0.77      0.77      0.77       924\nweighted avg       0.78      0.78      0.78       924\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import udf\n\ndef create_malayalam_dataset(content_col, sentiment_col, max_len=128):\n    tokenizer = BertTokenizer.from_pretrained('l3cube-pune/malayalam-bert')\n\n    def tokenize_text(content, sentiment):\n        inputs = tokenizer.encode_plus(\n            content,\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            return_token_type_ids=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'text': content,\n            'input_ids': inputs['input_ids'].flatten().tolist(),\n            'attention_mask': inputs['attention_mask'].flatten().tolist(),\n            'target': int(sentiment)\n        }\n\n    return udf(tokenize_text, StructType([\n        StructField('text', StringType(), True),\n        StructField('input_ids', ArrayType(IntegerType()), True),\n        StructField('attention_mask', ArrayType(IntegerType()), True),\n        StructField('target', IntegerType(), True)\n    ]))\n\ndef main():\n    # Create a SparkSession\n    spark = SparkSession.builder \\\n        .appName(\"Malayalam Tweets Analysis\") \\\n        .getOrCreate()\n\n    # Read the CSV files into a Spark DataFrame\n    dfs_spark = []\n    directory = '/kaggle/input/malayalam-tweets/'\n    for filename in os.listdir(directory):\n        if filename.endswith(\".csv\") and \"total\" not in filename:\n            file_path = os.path.join(directory, filename)\n            df_spark = spark.read.csv(file_path, header=True, inferSchema=True)\n            dfs_spark.append(df_spark)\n\n    # Combine all Spark DataFrames into a single Spark DataFrame\n    df_spark = reduce(lambda df1, df2: df1.union(df2), dfs_spark)\n    \n    df_spark = spark.createDataFrame(df)\n\n    # Define UDF to create Malayalam Dataset\n    create_dataset_udf = create_malayalam_dataset(\"clean_content\", \"sentiment\")\n\n    # Apply UDF to create train_dataset and val_dataset\n    df_spark = df_spark.withColumn(\"malayalam_dataset\", create_dataset_udf(df_spark[\"clean_content\"], df_spark[\"sentiment\"]))\n\n    # Split dataset into train and validation\n    train_df, val_df = df_spark.randomSplit([0.9, 0.1], seed=42)\n\n    # Convert Spark DataFrame to pandas DataFrame (optional)\n    train_df_pandas = train_df.select(\"malayalam_dataset\").toPandas()\n    val_df_pandas = val_df.select(\"malayalam_dataset\").toPandas()\n\n    # Convert pandas DataFrames to datasets\n    train_dataset = [row.malayalam_dataset for _, row in train_df_pandas.iterrows()]\n    val_dataset = [row.malayalam_dataset for _, row in val_df_pandas.iterrows()]\n\n    # Other steps (not included in this snippet)\n    # Initialize model, optimizer, scheduler\n    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    # val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    # train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T10:25:51.843607Z","iopub.execute_input":"2024-05-04T10:25:51.843996Z","iopub.status.idle":"2024-05-04T10:25:51.858276Z","shell.execute_reply.started":"2024-05-04T10:25:51.843966Z","shell.execute_reply":"2024-05-04T10:25:51.856254Z"},"trusted":true},"execution_count":8,"outputs":[]}]}
