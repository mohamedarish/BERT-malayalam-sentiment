{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iRmq7IpTyZUT"
      },
      "outputs": [],
      "source": [
        "# @title Google Drive Link\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdDaOKuEE299"
      },
      "outputs": [],
      "source": [
        "# @title Read csv file\n",
        "done = [\n",
        "    \"അതിഗംഭീര\",\n",
        "    \"അഭിനന്ദന\",\n",
        "    \"അഭിവൃദ്ധി\",\n",
        "    \"ആത്മവിശ്വാസം\",\n",
        "    \"ആരോപണ\",\n",
        "    \"ഇഷ്ട\",\n",
        "    \"ഉത്തമ\",\n",
        "    \"കഷ്ട\",\n",
        "    \"പരാജയ\",\n",
        "    \"പ്രവൃത്തി\",\n",
        "    \"വിശ്വാസം\",\n",
        "    \"സമാധാനം\",\n",
        "    \"സാഹചര്യം\",\n",
        "    \"സ്നേഹ\",\n",
        "    \"ആഘാത\",\n",
        "    \"അഭിമാന\",\n",
        "    \"ഉന്മേഷം\",\n",
        "    \"വിജയം\",\n",
        "    \"ശാന്തം\",\n",
        "    \"സ്ത്രീ\",\n",
        "    \"ബാധകം\",\n",
        "    \"വിജ്ഞാനം\",\n",
        "    \"കഴിവും\",\n",
        "    \"സാക്ഷരത\",\n",
        "    # \"അനുഭൂതി\", # This is invalid\n",
        "    \"അംഗീക\",\n",
        "    \"നേരായ\",\n",
        "    # \"സ്വാതന്ത്ര്യം\", # This is invalid\n",
        "    \"\"\n",
        "]\n",
        "\n",
        "import glob\n",
        "\n",
        "g = glob.glob(\"/content/drive/MyDrive/Dataset/new data/*.csv\")\n",
        "\n",
        "not_done = []\n",
        "\n",
        "for file in g:\n",
        "    yes = False\n",
        "\n",
        "    for d in done:\n",
        "        if d in file:\n",
        "            yes = True\n",
        "            continue\n",
        "\n",
        "    if not yes:\n",
        "        not_done.append(file)\n",
        "\n",
        "print(not_done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGsQaAPZ7KK0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = \"സ്വാതന്ത്ര്യം\"\n",
        "\n",
        "csv_df = pd.read_csv(f\"/content/drive/MyDrive/Dataset/new data/{filename}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-ncE5inSisd"
      },
      "outputs": [],
      "source": [
        "# @title Remove duplicates\n",
        "print(\"old count\", csv_df.count())\n",
        "\n",
        "#drop_duplicates for drop duplicate values\n",
        "csv_df = csv_df.drop_duplicates(['content'], keep='first')\n",
        "\n",
        "print(\"new count\", csv_df.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dngG1yiN0ME6"
      },
      "outputs": [],
      "source": [
        "# @title Clean the data\n",
        "import re\n",
        "\n",
        "def remove_usertags_hashtags_and_urls(tweet):\n",
        "    if isinstance(tweet, str):\n",
        "        new_line_pattern = re.compile(r'\\\\n')\n",
        "        no_new_line = new_line_pattern.sub(\" \", tweet)\n",
        "\n",
        "        show_more_pattern = re.compile(r'Show more\\s*$')\n",
        "        no_show_more = show_more_pattern.sub(\"\", no_new_line)\n",
        "\n",
        "        usertag_pattern = re.compile(r'@[^\\s]+')\n",
        "        no_usertags = usertag_pattern.sub(\"\", no_show_more)\n",
        "\n",
        "        hashtag_pattern = re.compile(r'#[^\\s]+')\n",
        "        no_hashtags = hashtag_pattern.sub(\"\", no_usertags)\n",
        "        # Some people do not know that hashtag followed by a space does not create a hashtag and hence we need to remove the hashtags appearing alone\n",
        "        dumb_hashtag_pattern = re.compile(r'#')\n",
        "        no_dumb_hashtags = dumb_hashtag_pattern.sub(\"\", no_hashtags)\n",
        "\n",
        "        url_pattern = re.compile(r'https?://[^\\s]+')\n",
        "        no_urls = url_pattern.sub(\"\", no_dumb_hashtags)\n",
        "\n",
        "        no_urls = no_urls.strip()\n",
        "\n",
        "        return no_urls\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "csv_df['clean_content'] = csv_df['content'].apply(remove_usertags_hashtags_and_urls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjJ74SOYHxiQ"
      },
      "outputs": [],
      "source": [
        "# @title Translate the tweets and store it in english column\n",
        "import requests, uuid, json\n",
        "\n",
        "def translate_tweet(tweet):\n",
        "    # Add your key and endpoint\n",
        "    key = \"9c0bfabc26264af690254eef3ff6a81a\" # Minhaj Key Feb done 2M\n",
        "    # key = \"5cbd5d35830e4409ae9c2d072d58b5f7\"\n",
        "    endpoint = \"https://api.cognitive.microsofttranslator.com/\"\n",
        "\n",
        "    # location, also known as region.\n",
        "    # required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n",
        "    location = \"southeastasia\"\n",
        "\n",
        "    path = '/translate'\n",
        "    constructed_url = endpoint + path\n",
        "\n",
        "    params = {\n",
        "        'api-version': '3.0',\n",
        "        'from': 'ml',\n",
        "        'to': ['en']\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': key,\n",
        "        # location required if you're using a multi-service or regional (not global) resource.\n",
        "        'Ocp-Apim-Subscription-Region': location,\n",
        "        'Content-type': 'application/json',\n",
        "        'X-ClientTraceId': str(uuid.uuid4())\n",
        "    }\n",
        "\n",
        "    # You can pass more than one object in body.\n",
        "    body = [{\n",
        "        'text': tweet\n",
        "    }]\n",
        "\n",
        "    request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "\n",
        "    response = request.json()\n",
        "\n",
        "    print(response)\n",
        "\n",
        "    if (isinstance(response, dict)):\n",
        "        return \"rate limited\"\n",
        "\n",
        "    if len(response) > 0 and response[0] and len(response[0][\"translations\"]) > 0:\n",
        "        return response[0][\"translations\"][0][\"text\"]\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "csv_df['english'] = csv_df['clean_content'].apply(translate_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tDbFydaAS6n"
      },
      "outputs": [],
      "source": [
        "# @title Fix if any translations have been missed\n",
        "for (index, row) in csv_df.iterrows():\n",
        "    content = row['clean_content']\n",
        "    if (row['english'] == \"rate limited\") or (row['english'] == \"\"):\n",
        "        row['english'] = translate_tweet(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOnwD50REJ-p"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece] torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm8FUwO-qmCO"
      },
      "outputs": [],
      "source": [
        "# @title Get sentiment\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"MarieAngeA13/Sentiment-Analysis-BERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "def get_sentiment(english_tweet):\n",
        "\n",
        "    encoded_text = tokenizer(english_tweet, return_tensors=\"pt\")  # Tokenize and convert to tensors\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded_text)  # Forward pass through the model\n",
        "        logits = outputs.logits  # Extract logits (model predictions)\n",
        "        predictions = torch.argmax(logits, dim=-1)  # Get the class labels\n",
        "\n",
        "        # Since this model outputs numerical labels, map them back to sentiment classes\n",
        "        label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "        predicted_sentiment = label_map[predictions.item()]  # Get the predicted sentiment\n",
        "\n",
        "        print(f\"Predicted sentiment: {predicted_sentiment}\")\n",
        "\n",
        "        sentiment_table = {\n",
        "            'Positive': 1,\n",
        "            'Neutral': 0,\n",
        "            'Negative': -1\n",
        "        }\n",
        "\n",
        "        return sentiment_table[predicted_sentiment]\n",
        "\n",
        "csv_df['sentiment'] = csv_df['english'].apply(get_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj4oDhStF9ZX"
      },
      "outputs": [],
      "source": [
        "# @title Create a new csv with the joined content with all new columns, the new file shall be named month(first 3 letters)_day.csv, Eg: Feb_24\n",
        "from datetime import datetime\n",
        "\n",
        "today = datetime.today()\n",
        "month_abbreviation = today.strftime(\"%b\")\n",
        "day = today.strftime(\"%d\")\n",
        "\n",
        "# csv_df.to_csv(f\"/content/drive/MyDrive/Dataset/new data/{month_abbreviation}_{day}.csv\", index=False)\n",
        "csv_df.to_csv(f\"/content/drive/MyDrive/Dataset/scored/{filename}.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}