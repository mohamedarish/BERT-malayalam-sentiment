{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8223486,"sourceType":"datasetVersion","datasetId":4875499}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mohamedarish/with-dataset-as-input?scriptVersionId=175059668\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pyspark transformers torch tqdm scikit-learn sparknlp huggingface_hub fasttext","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOGISTIC REGRESSION","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nfrom huggingface_hub import hf_hub_download\nimport fasttext\nimport fasttext.util\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nlogistic_regression_model = LogisticRegression(max_iter=1000)\n\n# Step 6: Train Model in Epochs\nepochs = 10\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    logistic_regression_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, logistic_regression_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, logistic_regression_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, logistic_regression_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, logistic_regression_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RANDOM FOREST","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport fasttext.util\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nrandom_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Step 6: Train Model in Epochs\nepochs = 10\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    random_forest_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, random_forest_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, random_forest_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, random_forest_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, random_forest_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport fasttext\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-ml-vectors\", filename=\"model.bin\")\nmodel = fasttext.load_model(model_path)\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n            dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\ndf = df.dropna(subset=['clean_content'])\n\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Step 3: Tokenization and Feature Extraction\ndef extract_features(text):\n    tokens = text.split()  # Tokenize text into words\n    vector_sum = sum(model.get_sentence_vector(token) for token in tokens)  # Get FastText vector for each word and sum them\n    return vector_sum / len(tokens)  # Average the word vectors to get text vector\n\n# Apply feature extraction to each text in the dataset\ndf['text_vector'] = df['clean_content'].apply(extract_features)\n\n# Step 4: Split Data\nX_train, X_test, y_train, y_test = train_test_split(df['text_vector'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Step 5: Initialize Logistic Regression Model\nsvm_model = SVC(kernel='linear', random_state=42)\n\n# Step 6: Train Model in Epochs\nepochs = 10\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    tqdm_bar = tqdm(total=len(X_train), position=0, leave=True)  # Create tqdm progress bar\n    \n    # Train the model\n    svm_model.fit(X_train.tolist(), y_train)\n    \n    # Calculate training metrics\n    train_loss = None  # Logistic regression does not have a loss attribute\n    train_accuracy = accuracy_score(y_train, svm_model.predict(X_train.tolist()))\n    train_precision = precision_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    train_recall = recall_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    train_f1_score = f1_score(y_train, svm_model.predict(X_train.tolist()), average='weighted')\n    \n    # Calculate validation metrics\n    val_accuracy = accuracy_score(y_test, svm_model.predict(X_test.tolist()))\n    val_precision = precision_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    val_recall = recall_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    val_f1_score = f1_score(y_test, svm_model.predict(X_test.tolist()), average='weighted')\n    \n    tqdm_bar.close()  # Close tqdm progress bar\n    \n    # Print metrics\n    print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}, Train Precision: {train_precision}, Train Recall: {train_recall}, Train F1-score: {train_f1_score}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Precision: {val_precision}, Validation Recall: {val_recall}, Validation F1-score: {val_f1_score}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BIDIRECTIONAL LSTM WITH ATTENTION LAYER","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Activation, Lambda, RepeatVector, Permute, Flatten\nimport tensorflow.keras.backend as K\nimport os\nimport torch\n\ndef clear_gpu_memory():\n  \"\"\"Frees memory allocated on the GPU.\"\"\"\n  if torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# # Directory containing your CSV files\ndirectory = '/kaggle/input/malayalam-tweets/'\n\n# # List to store DataFrames from each CSV file\ndfs = []\n\n# # Loop through each file in the directory\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\"):\n        file_path = os.path.join(directory, filename)\n        # Read the CSV file into a DataFrame\n        a_df = pd.read_csv(file_path)\n\n        if \"datetimee\" in a_df.columns:\n            # print(\"has datetimeee\")\n            a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n            # Append the DataFrame to the list\n        dfs.append(a_df)\n\n# # Combine all DataFrames into a single DataFrame\ndf = pd.concat(dfs, ignore_index=True)\n\nprint(df.size)\n\ndf = df.dropna(subset=['clean_content'])\n\nprint(df.size)\ndf['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n# Preprocessing\nmaxlen = 10000  # Maximum sequence length\nmax_words = 200000  # Maximum number of words in vocabulary\n\n# Tokenize the text\ntokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(df['content'])\n\n# Convert text to sequences\nsequences = tokenizer.texts_to_sequences(df['content'])\n\n# Pad sequences to maxlen\nX = pad_sequences(sequences, maxlen=maxlen)\n\n# Label encoding for sentiments (assuming you have 'sentiment' column)\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df['sentiment'])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the Bidirectional LSTM with Attention Model\ninput_seq = Input(shape=(maxlen,))\nembedding = Embedding(max_words, 128, input_length=maxlen)(input_seq)\nlstm = Bidirectional(LSTM(64, return_sequences=True))(embedding)\n\n# Attention Mechanism\nattention = Dense(1, activation='tanh')(lstm)\nattention = Flatten()(attention)\nattention = Activation('softmax', name='attention_weights')(attention)\nattention = RepeatVector(128 * 2)(attention)\nattention = Permute([2, 1])(attention)\n\nsent_representation = Concatenate(axis=-1)([lstm, attention])\nsent_representation = Lambda(lambda x: K.sum(x, axis=1))(sent_representation)\n\noutput = Dense(1, activation='sigmoid')(sent_representation)\n\nmodel = Model(inputs=input_seq, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.1)\n\nclear_gpu_memory()\n\n# Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy}')\n\n# Print training and validation metrics\nprint(\"Training Loss:\", history.history['loss'])\nprint(\"Training Accuracy:\", history.history['accuracy'])\nprint(\"Validation Loss:\", history.history['val_loss'])\nprint(\"Validation Accuracy:\", history.history['val_accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom tqdm import tqdm\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport os\nimport pandas as pd\nimport torch\n\ndef clear_gpu_memory():\n  \"\"\"Frees memory allocated on the GPU.\"\"\"\n  if torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Load Malayalam BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('l3cube-pune/malayalam-bert')\n\n# Define dataset class\nclass MalayalamDataset(Dataset):\n    def __init__(self, dataframe, max_len=128):\n        self.data = dataframe\n        self.max_len = max_len\n        self.texts = self.data.content.tolist()\n        self.targets = self.data.sentiment.tolist()\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = str(self.texts[index])\n        target = self.targets[index]\n\n        inputs = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'text': text,\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'target': torch.tensor(target, dtype=torch.long)\n        }\n\n# Load and preprocess dataset\ndef load_dataset(file_path):\n    df = pd.read_csv(file_path)\n    return df\n\n# Fine-tune BERT for sentiment analysis\ndef train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        correct = 0\n        total_targets = 0\n        for batch in tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n            correct += (preds == targets.cpu().numpy()).sum()\n            total_targets += len(targets)\n\n            loss.backward()\n            optimizer.step()\n\n        avg_train_loss = total_loss / len(train_loader)\n        train_accuracy = correct / total_targets\n        print(f\"Average training loss for epoch {epoch+1}: {avg_train_loss}\")\n        print(f\"Training Accuracy for epoch {epoch+1}: {train_accuracy}\")\n\n        val_loss, val_acc, val_report = evaluate_model(model, val_loader, device)\n        print(f\"Validation loss: {val_loss}, Accuracy: {val_acc}\")\n        print(\"Validation Report:\")\n        print(val_report)\n\n        scheduler.step()\n\n# Evaluate the model\ndef evaluate_model(model, val_loader, device):\n    model.eval()\n    val_targets = []\n    val_outputs = []\n\n    total_val_loss = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            targets = batch['target'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n\n            val_targets.extend(targets.cpu().numpy())\n            val_outputs.extend(preds)\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_targets, val_outputs)\n    val_report = classification_report(val_targets, val_outputs, target_names=[\"Negative\", \"Neutral\", \"Positive\"])\n    return avg_val_loss, val_accuracy, val_report\n# Main function to train and evaluate\ndef main():\n  tokenizer.save_pretrained(\"bert-tokenizer\")\n\n#   return\n  # # Directory containing your CSV files\n  directory = '/kaggle/input/malayalam-tweets/'\n\n  # # List to store DataFrames from each CSV file\n  dfs = []\n\n  # # Loop through each file in the directory\n  for filename in os.listdir(directory):\n    if filename.endswith(\".csv\") and \"total\" not in filename:\n      file_path = os.path.join(directory, filename)\n      # Read the CSV file into a DataFrame\n      a_df = pd.read_csv(file_path)\n\n      if \"datetimee\" in a_df.columns:\n        # print(\"has datetimeee\")\n        a_df = a_df.rename(columns={\"datetimee\": \"datetime\"})\n      # Append the DataFrame to the list\n      dfs.append(a_df)\n\n  # # Combine all DataFrames into a single DataFrame\n  df = pd.concat(dfs, ignore_index=True)\n\n  df = df.dropna(subset=['clean_content'])\n\n  df['sentiment'] = df['sentiment'].map({-1: 0, 0: 1, 1: 2})\n\n  # Split dataset into train and validation\n  train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n\n  # Create datasets and dataloaders\n  train_dataset = MalayalamDataset(train_df)\n  val_dataset = MalayalamDataset(val_df)\n\n  batch_size = 8\n  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n  # Load pre-trained BERT model for sequence classification\n  model = BertForSequenceClassification.from_pretrained('l3cube-pune/malayalam-bert', num_labels=3)\n\n  # Send model to GPU, if available\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  model.to(device)\n\n  # Create optimizer\n  optimizer = AdamW(model.parameters(), lr=2e-5)\n\n  # Create scheduler\n  scheduler = ExponentialLR(optimizer, gamma=0.9)\n\n  # Train the model\n  train_model(train_loader, val_loader, model, optimizer, scheduler, device, epochs=9)\n\n  clear_gpu_memory()\n\n  # Save the trained model\n  model.save_pretrained(\"malayalam_sentiment_model\")\n\nif __name__ == \"__main__\":\n  main()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]}]}